[{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement codeofconduct@posit.co. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 textrecipes authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/articles/Working-with-n-grams.html","id":"only-using-step_tokenize","dir":"Articles","previous_headings":"","what":"Only using step_tokenize()","title":"Working with n-grams","text":"first methods work using n-gram token one built-engine step_tokenize() get full list available tokens type ?step_tokenize() go Details. can use token=\"ngrams\" along engine = \"tokenizers\"(default) tokenize n-grams. finish recipe() step_tokenfilter() step_tf(). filtering doesn’t anything data size good practice use step_tokenfilter() using step_tf() step_tfidf() control size resulting data.frame. need pass arguments underlying tokenizer function can pass named list options argument step_tokenize() Lastly can also supply custom tokenizer step_tokenize() using custom_token argument. Pros: uses 1 step Simple use Cons: Minimal flexibility, (tokenizers::tokenize_ngrams() don’t let control words tokenized.) able tune number tokens n-gram","code":"abc_tibble <- tibble(text = abc)  rec <- recipe(~text, data = abc_tibble) %>%   step_tokenize(text, token = \"ngrams\") %>%   step_tokenfilter(text) %>%   step_tf(text)  abc_ngram <- rec %>%   prep() %>%   bake(new_data = NULL) #> Warning: max_tokens was set to '100', but only 14 was available and #> selected.  abc_ngram #> # A tibble: 2 × 14 #>   `tf_text_a place where` `tf_text_an insect that` `tf_text_bank is a` #>                     <int>                    <int>               <int> #> 1                       1                        0                   1 #> 2                       0                        1                   0 #> # ℹ 11 more variables: `tf_text_bee is an` <int>, #> #   `tf_text_insect that gathers` <int>, `tf_text_is a place` <int>, #> #   `tf_text_is an insect` <int>, `tf_text_place where you` <int>, #> #   `tf_text_put your money` <int>, `tf_text_that gathers honey` <int>, #> #   `tf_text_the bank is` <int>, `tf_text_the bee is` <int>, #> #   `tf_text_where you put` <int>, `tf_text_you put your` <int>  names(abc_ngram) #>  [1] \"tf_text_a place where\"       \"tf_text_an insect that\"      #>  [3] \"tf_text_bank is a\"           \"tf_text_bee is an\"           #>  [5] \"tf_text_insect that gathers\" \"tf_text_is a place\"          #>  [7] \"tf_text_is an insect\"        \"tf_text_place where you\"     #>  [9] \"tf_text_put your money\"      \"tf_text_that gathers honey\"  #> [11] \"tf_text_the bank is\"         \"tf_text_the bee is\"          #> [13] \"tf_text_where you put\"       \"tf_text_you put your\" abc_tibble <- tibble(text = abc)  rec <- recipe(~text, data = abc_tibble) %>%   step_tokenize(text, token = \"ngrams\", options = list(     n = 2,     ngram_delim = \"_\"   )) %>%   step_tokenfilter(text) %>%   step_tf(text)  abc_ngram <- rec %>%   prep() %>%   bake(new_data = NULL) #> Warning: max_tokens was set to '100', but only 16 was available and #> selected.  abc_ngram #> # A tibble: 2 × 16 #>   tf_text_a_place tf_text_an_insect tf_text_bank_is tf_text_bee_is #>             <int>             <int>           <int>          <int> #> 1               1                 0               1              0 #> 2               0                 1               0              1 #> # ℹ 12 more variables: tf_text_gathers_honey <int>, #> #   tf_text_insect_that <int>, tf_text_is_a <int>, tf_text_is_an <int>, #> #   tf_text_place_where <int>, tf_text_put_your <int>, #> #   tf_text_that_gathers <int>, tf_text_the_bank <int>, #> #   tf_text_the_bee <int>, tf_text_where_you <int>, #> #   tf_text_you_put <int>, tf_text_your_money <int>  names(abc_ngram) #>  [1] \"tf_text_a_place\"       \"tf_text_an_insect\"     #>  [3] \"tf_text_bank_is\"       \"tf_text_bee_is\"        #>  [5] \"tf_text_gathers_honey\" \"tf_text_insect_that\"   #>  [7] \"tf_text_is_a\"          \"tf_text_is_an\"         #>  [9] \"tf_text_place_where\"   \"tf_text_put_your\"      #> [11] \"tf_text_that_gathers\"  \"tf_text_the_bank\"      #> [13] \"tf_text_the_bee\"       \"tf_text_where_you\"     #> [15] \"tf_text_you_put\"       \"tf_text_your_money\" abc_tibble <- tibble(text = abc)  bigram <- function(x) {   tokenizers::tokenize_ngrams(x, lowercase = FALSE, n = 2, ngram_delim = \".\") }  rec <- recipe(~text, data = abc_tibble) %>%   step_tokenize(text, custom_token = bigram) %>%   step_tokenfilter(text) %>%   step_tf(text)  abc_ngram <- rec %>%   prep() %>%   bake(new_data = NULL) #> Warning: max_tokens was set to '100', but only 16 was available and #> selected.  abc_ngram #> # A tibble: 2 × 16 #>   tf_text_a.place tf_text_an.insect tf_text_Bank.is tf_text_Bee.is #>             <int>             <int>           <int>          <int> #> 1               1                 0               1              0 #> 2               0                 1               0              1 #> # ℹ 12 more variables: tf_text_gathers.honey <int>, #> #   tf_text_insect.that <int>, tf_text_is.a <int>, tf_text_is.an <int>, #> #   tf_text_place.where <int>, tf_text_put.your <int>, #> #   tf_text_that.gathers <int>, tf_text_The.Bank <int>, #> #   tf_text_The.Bee <int>, tf_text_where.you <int>, #> #   tf_text_you.put <int>, tf_text_your.money <int>  names(abc_ngram) #>  [1] \"tf_text_a.place\"       \"tf_text_an.insect\"     #>  [3] \"tf_text_Bank.is\"       \"tf_text_Bee.is\"        #>  [5] \"tf_text_gathers.honey\" \"tf_text_insect.that\"   #>  [7] \"tf_text_is.a\"          \"tf_text_is.an\"         #>  [9] \"tf_text_place.where\"   \"tf_text_put.your\"      #> [11] \"tf_text_that.gathers\"  \"tf_text_The.Bank\"      #> [13] \"tf_text_The.Bee\"       \"tf_text_where.you\"     #> [15] \"tf_text_you.put\"       \"tf_text_your.money\""},{"path":"https://textrecipes.tidymodels.org/dev/articles/Working-with-n-grams.html","id":"using-step_tokenize-and-step_ngram","dir":"Articles","previous_headings":"","what":"Using step_tokenize() and step_ngram()","title":"Working with n-grams","text":"version 0.2.0 can use step_ngram() along step_tokenize() gain higher control n-grams generated. Now able perform additional steps tokenization n-gram creation stemming tokens. also works great cases need higher flexibility want use powerful engine spacyr doesn’t come n-gram tokenizer. Furthermore num_tokens argument tunable dials tune package. Pros: Full flexibility Number tokens tunable Cons: 1 Additional step needed","code":"abc_tibble <- tibble(text = abc)  rec <- recipe(~text, data = abc_tibble) %>%   step_tokenize(text) %>%   step_ngram(text, num_tokens = 3) %>%   step_tokenfilter(text) %>%   step_tf(text)  abc_ngram <- rec %>%   prep() %>%   bake(new_data = NULL) #> Warning: max_tokens was set to '100', but only 14 was available and #> selected.  abc_ngram #> # A tibble: 2 × 14 #>   tf_text_a_place_where tf_text_an_insect_that tf_text_bank_is_a #>                   <int>                  <int>             <int> #> 1                     1                      0                 1 #> 2                     0                      1                 0 #> # ℹ 11 more variables: tf_text_bee_is_an <int>, #> #   tf_text_insect_that_gathers <int>, tf_text_is_a_place <int>, #> #   tf_text_is_an_insect <int>, tf_text_place_where_you <int>, #> #   tf_text_put_your_money <int>, tf_text_that_gathers_honey <int>, #> #   tf_text_the_bank_is <int>, tf_text_the_bee_is <int>, #> #   tf_text_where_you_put <int>, tf_text_you_put_your <int>  names(abc_ngram) #>  [1] \"tf_text_a_place_where\"       \"tf_text_an_insect_that\"      #>  [3] \"tf_text_bank_is_a\"           \"tf_text_bee_is_an\"           #>  [5] \"tf_text_insect_that_gathers\" \"tf_text_is_a_place\"          #>  [7] \"tf_text_is_an_insect\"        \"tf_text_place_where_you\"     #>  [9] \"tf_text_put_your_money\"      \"tf_text_that_gathers_honey\"  #> [11] \"tf_text_the_bank_is\"         \"tf_text_the_bee_is\"          #> [13] \"tf_text_where_you_put\"       \"tf_text_you_put_your\" abc_tibble <- tibble(text = abc)  rec <- recipe(~text, data = abc_tibble) %>%   step_tokenize(text) %>%   step_stem(text) %>%   step_ngram(text, num_tokens = 3) %>%   step_tokenfilter(text) %>%   step_tf(text)  abc_ngram <- rec %>%   prep() %>%   bake(new_data = NULL) #> Warning: max_tokens was set to '100', but only 14 was available and #> selected.  abc_ngram #> # A tibble: 2 × 14 #>   tf_text_a_place_where tf_text_an_insect_that tf_text_bank_i_a #>                   <int>                  <int>            <int> #> 1                     1                      0                1 #> 2                     0                      1                0 #> # ℹ 11 more variables: tf_text_bee_i_an <int>, tf_text_i_a_place <int>, #> #   tf_text_i_an_insect <int>, tf_text_insect_that_gather <int>, #> #   tf_text_place_where_you <int>, tf_text_put_your_monei <int>, #> #   tf_text_that_gather_honei <int>, tf_text_the_bank_i <int>, #> #   tf_text_the_bee_i <int>, tf_text_where_you_put <int>, #> #   tf_text_you_put_your <int>  names(abc_ngram) #>  [1] \"tf_text_a_place_where\"      \"tf_text_an_insect_that\"     #>  [3] \"tf_text_bank_i_a\"           \"tf_text_bee_i_an\"           #>  [5] \"tf_text_i_a_place\"          \"tf_text_i_an_insect\"        #>  [7] \"tf_text_insect_that_gather\" \"tf_text_place_where_you\"    #>  [9] \"tf_text_put_your_monei\"     \"tf_text_that_gather_honei\"  #> [11] \"tf_text_the_bank_i\"         \"tf_text_the_bee_i\"          #> [13] \"tf_text_where_you_put\"      \"tf_text_you_put_your\""},{"path":"https://textrecipes.tidymodels.org/dev/articles/cookbook---using-more-complex-recipes-involving-text.html","id":"counting-select-words","dir":"Articles","previous_headings":"","what":"Counting select words","title":"Cookbook - Using more complex recipes involving text","text":"Sometimes enough know counts handful specific words. can easily achieved using arguments custom_stopword_source keep = TRUE step_stopwords.","code":"words <- c(\"or\", \"and\", \"on\")  okc_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stopwords(medium, custom_stopword_source = words, keep = TRUE) %>%   step_tf(medium)  okc_obj <- okc_rec %>%   prep()  bake(okc_obj, tate_text) %>%   select(starts_with(\"tf_medium\")) #> # A tibble: 4,284 × 3 #>    tf_medium_and tf_medium_on tf_medium_or #>            <int>        <int>        <int> #>  1             1            0            1 #>  2             0            1            0 #>  3             0            1            0 #>  4             0            1            0 #>  5             0            1            0 #>  6             0            1            0 #>  7             0            1            0 #>  8             0            1            0 #>  9             1            1            0 #> 10             0            1            0 #> # ℹ 4,274 more rows"},{"path":"https://textrecipes.tidymodels.org/dev/articles/cookbook---using-more-complex-recipes-involving-text.html","id":"removing-words-in-addition-to-the-stop-words-list","dir":"Articles","previous_headings":"","what":"Removing words in addition to the stop words list","title":"Cookbook - Using more complex recipes involving text","text":"might know certain words don’t want included isn’t part stop word list choice. can easily done applying step_stopwords step twice, stop words special words.","code":"stopwords_list <- c(   \"was\", \"she's\", \"who\", \"had\", \"some\", \"same\", \"you\", \"most\",   \"it's\", \"they\", \"for\", \"i'll\", \"which\", \"shan't\", \"we're\",   \"such\", \"more\", \"with\", \"there's\", \"each\" )  words <- c(\"sad\", \"happy\")  okc_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stopwords(medium, custom_stopword_source = stopwords_list) %>%   step_stopwords(medium, custom_stopword_source = words) %>%   step_tfidf(medium)  okc_obj <- okc_rec %>%   prep()  bake(okc_obj, tate_text) %>%   select(starts_with(\"tfidf_medium\")) #> # A tibble: 4,284 × 951 #>    tfidf_medium_1 tfidf_medium_10 tfidf_medium_100 tfidf_medium_11 #>             <dbl>           <dbl>            <dbl>           <dbl> #>  1              0               0                0               0 #>  2              0               0                0               0 #>  3              0               0                0               0 #>  4              0               0                0               0 #>  5              0               0                0               0 #>  6              0               0                0               0 #>  7              0               0                0               0 #>  8              0               0                0               0 #>  9              0               0                0               0 #> 10              0               0                0               0 #> # ℹ 4,274 more rows #> # ℹ 947 more variables: tfidf_medium_12 <dbl>, tfidf_medium_13 <dbl>, #> #   tfidf_medium_133 <dbl>, tfidf_medium_14 <dbl>, tfidf_medium_15 <dbl>, #> #   tfidf_medium_151 <dbl>, tfidf_medium_16 <dbl>, #> #   tfidf_medium_160 <dbl>, tfidf_medium_16mm <dbl>, #> #   tfidf_medium_18 <dbl>, tfidf_medium_19 <dbl>, tfidf_medium_2 <dbl>, #> #   tfidf_medium_20 <dbl>, tfidf_medium_2000 <dbl>, …"},{"path":"https://textrecipes.tidymodels.org/dev/articles/cookbook---using-more-complex-recipes-involving-text.html","id":"letter-distributions","dir":"Articles","previous_headings":"","what":"Letter distributions","title":"Cookbook - Using more complex recipes involving text","text":"Another thing one might want look use different letters certain text. can use built-character tokenizer keep characters using step_stopwords step.","code":"okc_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium, token = \"characters\") %>%   step_stopwords(medium, custom_stopword_source = letters, keep = TRUE) %>%   step_tf(medium)  okc_obj <- okc_rec %>%   prep()  bake(okc_obj, tate_text) %>%   select(starts_with(\"tf_medium\")) #> # A tibble: 4,284 × 26 #>    tf_medium_a tf_medium_b tf_medium_c tf_medium_d tf_medium_e tf_medium_f #>          <int>       <int>       <int>       <int>       <int>       <int> #>  1           1           0           2           3           4           0 #>  2           1           0           1           0           2           0 #>  3           1           0           1           0           2           0 #>  4           1           0           1           0           2           0 #>  5           3           0           1           0           0           0 #>  6           3           0           1           0           0           0 #>  7           3           0           2           0           1           0 #>  8           1           0           1           1           1           0 #>  9           5           0           1           1           0           0 #> 10           1           0           0           0           1           0 #> # ℹ 4,274 more rows #> # ℹ 20 more variables: tf_medium_g <int>, tf_medium_h <int>, #> #   tf_medium_i <int>, tf_medium_j <int>, tf_medium_k <int>, #> #   tf_medium_l <int>, tf_medium_m <int>, tf_medium_n <int>, #> #   tf_medium_o <int>, tf_medium_p <int>, tf_medium_q <int>, #> #   tf_medium_r <int>, tf_medium_s <int>, tf_medium_t <int>, #> #   tf_medium_u <int>, tf_medium_v <int>, tf_medium_w <int>, …"},{"path":"https://textrecipes.tidymodels.org/dev/articles/cookbook---using-more-complex-recipes-involving-text.html","id":"tf-idf-of-ngrams-of-stemmed-tokens","dir":"Articles","previous_headings":"","what":"TF-IDF of ngrams of stemmed tokens","title":"Cookbook - Using more complex recipes involving text","text":"Sometimes fairly complicated computations. like term frequency inverse document frequency (TF-IDF) common 500 ngrams done stemmed tokens. quite handful seldom included option libraries. modularity textrecipes makes task fairly easy. First tokenize according words, stemming words. paste together stemmed tokens using step_untokenize back string tokenize time using ngram tokenizers. Lastly just filtering tfidf usual.","code":"okc_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium, token = \"words\") %>%   step_stem(medium) %>%   step_untokenize(medium) %>%   step_tokenize(medium, token = \"ngrams\") %>%   step_tokenfilter(medium, max_tokens = 500) %>%   step_tfidf(medium)  okc_obj <- okc_rec %>%   prep()  bake(okc_obj, tate_text) %>%   select(starts_with(\"tfidf_medium\")) #> # A tibble: 4,284 × 499 #>    tfidf_medium_100 digit …¹ tfidf_medium_16 mm b…² tfidf_medium_16 mm p…³ #>                        <dbl>                  <dbl>                  <dbl> #>  1                         0                      0                      0 #>  2                         0                      0                      0 #>  3                         0                      0                      0 #>  4                         0                      0                      0 #>  5                         0                      0                      0 #>  6                         0                      0                      0 #>  7                         0                      0                      0 #>  8                         0                      0                      0 #>  9                         0                      0                      0 #> 10                         0                      0                      0 #> # ℹ 4,274 more rows #> # ℹ abbreviated names: ¹​`tfidf_medium_100 digit print`, #> #   ²​`tfidf_medium_16 mm black`, ³​`tfidf_medium_16 mm project` #> # ℹ 496 more variables: `tfidf_medium_16 mm shown` <dbl>, #> #   `tfidf_medium_16mm shown a` <dbl>, #> #   `tfidf_medium_2 aluminium panel` <dbl>, #> #   `tfidf_medium_2 digit print` <dbl>, …"},{"path":"https://textrecipes.tidymodels.org/dev/articles/tokenlist.html","id":"tokens-attribute","dir":"Articles","previous_headings":"","what":"tokens attribute","title":"Under the hood - tokenlist","text":"tokens attribute vector unique tokens contained data list. attribute calculated automatically using tokenlist(). function applied tokenlist resulting unique tokens can derived new_tokenlist() can used create tokenlist known tokens attribute.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/articles/tokenlist.html","id":"lemma-and-pos-attributes","dir":"Articles","previous_headings":"","what":"lemma and pos attributes","title":"Under the hood - tokenlist","text":"lemma pos attribute used way. default NULL can filled depending engine used step_tokenize(). attribute list characters exact shape size tokenlist one--one relationship. specific element removed tokenlist corresponding element lemma pos removed.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Emil Hvitfeldt. Author, maintainer. . Copyright holder, funder.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hvitfeldt E (2023). textrecipes: Extra 'Recipes' Text Processing. https://github.com/tidymodels/textrecipes, https://textrecipes.tidymodels.org/.","code":"@Manual{,   title = {textrecipes: Extra 'Recipes' for Text Processing},   author = {Emil Hvitfeldt},   year = {2023},   note = {https://github.com/tidymodels/textrecipes, https://textrecipes.tidymodels.org/}, }"},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Extra Recipes for Text Processing","text":"textrecipes contain extra steps recipes package preprocessing text data.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Extra Recipes for Text Processing","text":"can install released version textrecipes CRAN : Install development version GitHub :","code":"install.packages(\"textrecipes\") # install.packages(\"pak\") pak::pak(\"tidymodels/textrecipes\")"},{"path":"https://textrecipes.tidymodels.org/dev/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Extra Recipes for Text Processing","text":"following example go steps needed, convert character variable TF-IDF tokenized words removing stopwords, , limiting ourself 10 used words. preprocessing conducted variable medium artist.","code":"library(recipes) library(textrecipes) library(modeldata)  data(\"tate_text\")  okc_rec <- recipe(~ medium + artist, data = tate_text) %>%   step_tokenize(medium, artist) %>%   step_stopwords(medium, artist) %>%   step_tokenfilter(medium, artist, max_tokens = 10) %>%   step_tfidf(medium, artist)  okc_obj <- okc_rec %>%   prep()  str(bake(okc_obj, tate_text)) #> tibble [4,284 × 20] (S3: tbl_df/tbl/data.frame) #>  $ tfidf_medium_colour     : num [1:4284] 2.31 0 0 0 0 ... #>  $ tfidf_medium_etching    : num [1:4284] 0 0.86 0.86 0.86 0 ... #>  $ tfidf_medium_gelatin    : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_medium_lithograph : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_medium_paint      : num [1:4284] 0 0 0 0 2.35 ... #>  $ tfidf_medium_paper      : num [1:4284] 0 0.422 0.422 0.422 0 ... #>  $ tfidf_medium_photograph : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_medium_print      : num [1:4284] 0 0 0 0 0 ... #>  $ tfidf_medium_screenprint: num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_medium_silver     : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_akram      : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_beuys      : num [1:4284] 0 0 0 0 0 ... #>  $ tfidf_artist_ferrari    : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_john       : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_joseph     : num [1:4284] 0 0 0 0 0 ... #>  $ tfidf_artist_león       : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_richard    : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_schütte    : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_thomas     : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ... #>  $ tfidf_artist_zaatari    : num [1:4284] 0 0 0 0 0 0 0 0 0 0 ..."},{"path":"https://textrecipes.tidymodels.org/dev/index.html","id":"breaking-changes","dir":"","previous_headings":"","what":"Breaking changes","title":"Extra Recipes for Text Processing","text":"version 0.4.0, step_lda() longer accepts character variables instead takes tokenlist variables. following recipe can replaced following recipe achive results","code":"recipe(~text_var, data = data) %>%   step_lda(text_var) lda_tokenizer <- function(x) text2vec::word_tokenizer(tolower(x)) recipe(~text_var, data = data) %>%   step_tokenize(text_var,     custom_token = lda_tokenizer   ) %>%   step_lda(text_var)"},{"path":"https://textrecipes.tidymodels.org/dev/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Extra Recipes for Text Processing","text":"project released Contributor Code Conduct. contributing project, agree abide terms. questions discussions tidymodels packages, modeling, machine learning, please post RStudio Community. think encountered bug, please submit issue. Either way, learn create share reprex (minimal, reproducible example), clearly communicate code. Check details contributing guidelines tidymodels packages get help.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/all_tokenized.html","id":null,"dir":"Reference","previous_headings":"","what":"Role Selection — all_tokenized","title":"Role Selection — all_tokenized","text":"all_tokenized() selects token variables, all_tokenized_predictors() selects predictor token variables.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/all_tokenized.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Role Selection — all_tokenized","text":"","code":"all_tokenized()  all_tokenized_predictors()"},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/emoji_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample sentences with emojis — emoji_samples","title":"Sample sentences with emojis — emoji_samples","text":"data set primarily used examples.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/emoji_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample sentences with emojis — emoji_samples","text":"","code":"emoji_samples"},{"path":"https://textrecipes.tidymodels.org/dev/reference/emoji_samples.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample sentences with emojis — emoji_samples","text":"tibble 1 column","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://textrecipes.tidymodels.org/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics required_pkgs, tidy, tunable","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/required_pkgs.step.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_clean_levels","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_clean_levels","text":"Recipe-adjacent packages always list required package steps can function properly within parallel processing schemes.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/required_pkgs.step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_clean_levels","text":"","code":"# S3 method for step_clean_levels required_pkgs(x, ...)  # S3 method for step_clean_names required_pkgs(x, ...)  # S3 method for step_dummy_hash required_pkgs(x, ...)  # S3 method for step_lda required_pkgs(x, ...)  # S3 method for step_lemma required_pkgs(x, ...)  # S3 method for step_ngram required_pkgs(x, ...)  # S3 method for step_pos_filter required_pkgs(x, ...)  # S3 method for step_sequence_onehot required_pkgs(x, ...)  # S3 method for step_stem required_pkgs(x, ...)  # S3 method for step_stopwords required_pkgs(x, ...)  # S3 method for step_text_normalization required_pkgs(x, ...)  # S3 method for step_textfeature required_pkgs(x, ...)  # S3 method for step_texthash required_pkgs(x, ...)  # S3 method for step_tf required_pkgs(x, ...)  # S3 method for step_tfidf required_pkgs(x, ...)  # S3 method for step_tokenfilter required_pkgs(x, ...)  # S3 method for step_tokenize required_pkgs(x, ...)  # S3 method for step_tokenize_bpe required_pkgs(x, ...)  # S3 method for step_tokenize_sentencepiece required_pkgs(x, ...)  # S3 method for step_tokenize_wordpiece required_pkgs(x, ...)  # S3 method for step_tokenmerge required_pkgs(x, ...)  # S3 method for step_untokenize required_pkgs(x, ...)  # S3 method for step_word_embeddings required_pkgs(x, ...)"},{"path":"https://textrecipes.tidymodels.org/dev/reference/required_pkgs.step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_clean_levels","text":"x recipe step","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/required_pkgs.step.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 methods for tracking which additional packages are needed for steps. — required_pkgs.step_clean_levels","text":"character vector","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/show_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Show token output of recipe — show_tokens","title":"Show token output of recipe — show_tokens","text":"Returns tokens list character vector recipe. function can useful diagnostics recipe construction used final recipe steps. Note function prep() bake() recipe used .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/show_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show token output of recipe — show_tokens","text":"","code":"show_tokens(rec, var, n = 6L)"},{"path":"https://textrecipes.tidymodels.org/dev/reference/show_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show token output of recipe — show_tokens","text":"rec recipe object var name variable n Number elements return.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/show_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show token output of recipe — show_tokens","text":"list character vectors","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/show_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show token output of recipe — show_tokens","text":"","code":"text_tibble <- tibble(text = c(\"This is words\", \"They are nice!\"))  recipe(~text, data = text_tibble) %>%   step_tokenize(text) %>%   show_tokens(text) #> [[1]] #> [1] \"this\"  \"is\"    \"words\" #>  #> [[2]] #> [1] \"they\" \"are\"  \"nice\" #>   library(modeldata) data(tate_text)  recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   show_tokens(medium) #> [[1]] #> [1] \"video\"      \"monitor\"    \"or\"         \"projection\" \"colour\"     #> [6] \"and\"        \"sound\"      \"stereo\"     #>  #> [[2]] #> [1] \"etching\" \"on\"      \"paper\"   #>  #> [[3]] #> [1] \"etching\" \"on\"      \"paper\"   #>  #> [[4]] #> [1] \"etching\" \"on\"      \"paper\"   #>  #> [[5]] #> [1] \"oil\"    \"paint\"  \"on\"     \"canvas\" #>  #> [[6]] #> [1] \"oil\"    \"paint\"  \"on\"     \"canvas\" #>"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Categorical Levels — step_clean_levels","title":"Clean Categorical Levels — step_clean_levels","text":"step_clean_levels creates specification recipe step clean nominal data (character factor) levels consist letters, numbers, underscore.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Categorical Levels — step_clean_levels","text":"","code":"step_clean_levels(   recipe,   ...,   role = NA,   trained = FALSE,   clean = NULL,   skip = FALSE,   id = rand_id(\"clean_levels\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Categorical Levels — step_clean_levels","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. clean named character vector clean recode categorical levels. NULL computed recipes::prep.recipe(). Note original variable character vector, converted factor. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Categorical Levels — step_clean_levels","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean Categorical Levels — step_clean_levels","text":"new levels cleaned reset dplyr::recode_factor(). data processed contains novel levels (.e., contained training set), converted missing.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Clean Categorical Levels — step_clean_levels","text":"tidy() step, tibble columns terms (selectors variables selected), original (original levels) value (cleaned levels) returned.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Clean Categorical Levels — step_clean_levels","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_levels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Categorical Levels — step_clean_levels","text":"","code":"library(recipes) library(modeldata) data(Smithsonian)  smith_tr <- Smithsonian[1:15, ] smith_te <- Smithsonian[16:20, ]  rec <- recipe(~., data = smith_tr)  rec <- rec %>%   step_clean_levels(name) rec <- prep(rec, training = smith_tr)  cleaned <- bake(rec, smith_tr)  tidy(rec, number = 1) #> # A tibble: 15 × 4 #>    terms original                                              value id    #>    <chr> <chr>                                                 <chr> <chr> #>  1 name  Anacostia Community Museum                            anac… clea… #>  2 name  Arthur M. Sackler Gallery                             arth… clea… #>  3 name  Arts and Industries Building                          arts… clea… #>  4 name  Cooper Hewitt, Smithsonian Design Museum              coop… clea… #>  5 name  Freer Gallery of Art                                  free… clea… #>  6 name  George Gustav Heye Center                             geor… clea… #>  7 name  Hirshhorn Museum and Sculpture Garden                 hirs… clea… #>  8 name  National Air and Space Museum                         nati… clea… #>  9 name  National Museum of African American History and Cult… nati… clea… #> 10 name  National Museum of African Art                        nati… clea… #> 11 name  National Museum of American History                   nati… clea… #> 12 name  National Museum of Natural History                    nati… clea… #> 13 name  National Museum of the American Indian                nati… clea… #> 14 name  National Portrait Gallery                             nati… clea… #> 15 name  Steven F. Udvar-Hazy Center                           stev… clea…  # novel levels are replaced with missing bake(rec, smith_te) #> # A tibble: 5 × 3 #>   name  latitude longitude #>   <fct>    <dbl>     <dbl> #> 1 NA        38.9     -77.0 #> 2 NA        38.9     -77.0 #> 3 NA        38.9     -77.0 #> 4 NA        38.9     -77.0 #> 5 NA        38.9     -77.1"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Variable Names — step_clean_names","title":"Clean Variable Names — step_clean_names","text":"step_clean_names creates specification recipe step clean variable names names consist letters, numbers, underscore.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Variable Names — step_clean_names","text":"","code":"step_clean_names(   recipe,   ...,   role = NA,   trained = FALSE,   clean = NULL,   skip = FALSE,   id = rand_id(\"clean_names\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Variable Names — step_clean_names","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. clean named character vector clean variable names. NULL computed recipes::prep.recipe(). skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Variable Names — step_clean_names","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Clean Variable Names — step_clean_names","text":"tidy() step, tibble columns terms (new clean variable names) value (original variable names).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Clean Variable Names — step_clean_names","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_clean_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Variable Names — step_clean_names","text":"","code":"library(recipes) data(airquality)  air_tr <- tibble(airquality[1:100, ]) air_te <- tibble(airquality[101:153, ])  rec <- recipe(~., data = air_tr)  rec <- rec %>%   step_clean_names(all_predictors()) rec <- prep(rec, training = air_tr) tidy(rec, number = 1) #> # A tibble: 6 × 3 #>   terms   value   id                #>   <chr>   <chr>   <chr>             #> 1 ozone   Ozone   clean_names_D81VR #> 2 solar_r Solar.R clean_names_D81VR #> 3 wind    Wind    clean_names_D81VR #> 4 temp    Temp    clean_names_D81VR #> 5 month   Month   clean_names_D81VR #> 6 day     Day     clean_names_D81VR  bake(rec, air_tr) #> # A tibble: 100 × 6 #>    ozone solar_r  wind  temp month   day #>    <int>   <int> <dbl> <int> <int> <int> #>  1    41     190   7.4    67     5     1 #>  2    36     118   8      72     5     2 #>  3    12     149  12.6    74     5     3 #>  4    18     313  11.5    62     5     4 #>  5    NA      NA  14.3    56     5     5 #>  6    28      NA  14.9    66     5     6 #>  7    23     299   8.6    65     5     7 #>  8    19      99  13.8    59     5     8 #>  9     8      19  20.1    61     5     9 #> 10    NA     194   8.6    69     5    10 #> # ℹ 90 more rows bake(rec, air_te) #> # A tibble: 53 × 6 #>    ozone solar_r  wind  temp month   day #>    <int>   <int> <dbl> <int> <int> <int> #>  1   110     207   8      90     8     9 #>  2    NA     222   8.6    92     8    10 #>  3    NA     137  11.5    86     8    11 #>  4    44     192  11.5    86     8    12 #>  5    28     273  11.5    82     8    13 #>  6    65     157   9.7    80     8    14 #>  7    NA      64  11.5    79     8    15 #>  8    22      71  10.3    77     8    16 #>  9    59      51   6.3    79     8    17 #> 10    23     115   7.4    76     8    18 #> # ℹ 43 more rows"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":null,"dir":"Reference","previous_headings":"","what":"Indicator Variables via Feature Hashing — step_dummy_hash","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"step_dummy_hash creates specification recipe step convert factors character columns series binary (signed binary) indicator columns.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"","code":"step_dummy_hash(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   signed = TRUE,   num_terms = 32L,   collapse = FALSE,   prefix = \"dummyhash\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"dummy_hash\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). signed logical, indicating whether use signed hash-function (generating values -1, 0, 1), reduce collisions hashing. Defaults TRUE. num_terms integer, number variables output. Defaults 32. collapse logical; selected columns collapsed single column create single set hashed features? prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"Feature hashing, hashing trick, transformation text variable new set numerical variables. done applying hashing function values factor levels using hash values feature indices. allows low memory representation data can helpful qualitative predictor many levels expected new levels prediction. implementation done using MurmurHash3 method. argument num_terms controls number indices hashing function map . tuning parameter transformation. Since hashing function can map two different tokens index, higher value num_terms result lower chance collision. new components names begin prefix, name variable, followed tokens separated -. variable names padded zeros. example prefix = \"hash\", num_terms < 10, names hash1 - hash9. num_terms = 101, names hash001 - hash101.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"tidy() step, tibble columns terms (selectors variables selected), value (whether signed hashing performed), num_terms (number terms), collapse (columns collapsed).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"step 2 tuning parameters: signed: Signed Hash Value (type: logical, default: TRUE) num_terms: # Hash Features (type: integer, default: 32)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"underlying operation allow case weights.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"Kilian Weinberger; Anirban Dasgupta; John Langford; Alex Smola; Josh Attenberg (2009). Kuhn Johnson (2019), Chapter 7, https://bookdown.org/max/FES/encoding-predictors--many-categories.html","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_dummy_hash.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Indicator Variables via Feature Hashing — step_dummy_hash","text":"","code":"library(recipes) library(modeldata) data(grants)  grants_rec <- recipe(~sponsor_code, data = grants_other) %>%   step_dummy_hash(sponsor_code)  grants_obj <- grants_rec %>%   prep() #> as(<dgTMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"CsparseMatrix\") instead  bake(grants_obj, grants_test) #> # A tibble: 518 × 32 #>    dummyhash_sponsor_code_01 dummyhash_sponsor_co…¹ dummyhash_sponsor_co…² #>                        <int>                  <int>                  <int> #>  1                         0                      0                      0 #>  2                         0                      0                      0 #>  3                         0                      0                      0 #>  4                         0                      1                      0 #>  5                         0                      0                      0 #>  6                         0                      1                      0 #>  7                         0                      0                      0 #>  8                         0                      0                      0 #>  9                         0                      0                      0 #> 10                         0                      1                      0 #> # ℹ 508 more rows #> # ℹ abbreviated names: ¹​dummyhash_sponsor_code_02, #> #   ²​dummyhash_sponsor_code_03 #> # ℹ 29 more variables: dummyhash_sponsor_code_04 <int>, #> #   dummyhash_sponsor_code_05 <int>, dummyhash_sponsor_code_06 <int>, #> #   dummyhash_sponsor_code_07 <int>, dummyhash_sponsor_code_08 <int>, #> #   dummyhash_sponsor_code_09 <int>, dummyhash_sponsor_code_10 <int>, …  tidy(grants_rec, number = 1) #> # A tibble: 1 × 5 #>   terms        value num_terms collapse id               #>   <chr>        <lgl>     <int> <lgl>    <chr>            #> 1 sponsor_code NA           NA NA       dummy_hash_KqQyW tidy(grants_obj, number = 1) #> # A tibble: 1 × 5 #>   terms        value num_terms collapse id               #>   <chr>        <lgl>     <int> <lgl>    <chr>            #> 1 sponsor_code TRUE         32 FALSE    dummy_hash_KqQyW"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate LDA Dimension Estimates of Tokens — step_lda","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"step_lda creates specification recipe step return lda dimension estimates text variable.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"","code":"step_lda(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   lda_models = NULL,   num_topics = 10L,   prefix = \"lda\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"lda\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"https://arxiv.org/abs/1301.3781","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). lda_models WarpLDA model object text2vec package. left NULL, default, train model based training data. Look examples fit WarpLDA model. num_topics integer desired number latent topics. prefix prefix generated column names, default \"lda\". keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"tidy() step, tibble columns terms (selectors variables selected) num_topics (number topics).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate LDA Dimension Estimates of Tokens — step_lda","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_lda(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL) %>%   slice(1:2) #> # A tibble: 2 × 14 #>      id artist          title  year lda_medium_1 lda_medium_2 lda_medium_3 #>   <dbl> <fct>           <fct> <dbl>        <dbl>        <dbl>        <dbl> #> 1 21926 Absalon         Prop…  1990            0       0.0143            0 #> 2 20472 Auerbach, Frank Mich…  1990            0       0                 0 #> # ℹ 7 more variables: lda_medium_4 <dbl>, lda_medium_5 <dbl>, #> #   lda_medium_6 <dbl>, lda_medium_7 <dbl>, lda_medium_8 <dbl>, #> #   lda_medium_9 <dbl>, lda_medium_10 <dbl> tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  num_topics id        #>   <chr>       <int> <chr>     #> 1 medium         10 lda_XOhek tidy(tate_obj, number = 2) #> # A tibble: 1 × 3 #>   terms  num_topics id        #>   <chr>       <int> <chr>     #> 1 medium         10 lda_XOhek  # Changing the number of topics. recipe(~., data = tate_text) %>%   step_tokenize(medium, artist) %>%   step_lda(medium, artist, num_topics = 20) %>%   prep() %>%   bake(new_data = NULL) %>%   slice(1:2) #> # A tibble: 2 × 43 #>      id title     year lda_medium_1 lda_medium_2 lda_medium_3 lda_medium_4 #>   <dbl> <fct>    <dbl>        <dbl>        <dbl>        <dbl>        <dbl> #> 1 21926 Proposa…  1990        0.157       0.0429            0       0.0143 #> 2 20472 Michael   1990        0           0                 0       0      #> # ℹ 36 more variables: lda_medium_5 <dbl>, lda_medium_6 <dbl>, #> #   lda_medium_7 <dbl>, lda_medium_8 <dbl>, lda_medium_9 <dbl>, #> #   lda_medium_10 <dbl>, lda_medium_11 <dbl>, lda_medium_12 <dbl>, #> #   lda_medium_13 <dbl>, lda_medium_14 <dbl>, lda_medium_15 <dbl>, #> #   lda_medium_16 <dbl>, lda_medium_17 <dbl>, lda_medium_18 <dbl>, #> #   lda_medium_19 <dbl>, lda_medium_20 <dbl>, lda_artist_1 <dbl>, #> #   lda_artist_2 <dbl>, lda_artist_3 <dbl>, lda_artist_4 <dbl>, …  # Supplying A pre-trained LDA model trained using text2vec library(text2vec) tokens <- word_tokenizer(tolower(tate_text$medium)) it <- itoken(tokens, ids = seq_along(tate_text$medium)) v <- create_vocabulary(it) dtm <- create_dtm(it, vocab_vectorizer(v)) lda_model <- LDA$new(n_topics = 15)  recipe(~., data = tate_text) %>%   step_tokenize(medium, artist) %>%   step_lda(medium, artist, lda_models = lda_model) %>%   prep() %>%   bake(new_data = NULL) %>%   slice(1:2) #> # A tibble: 2 × 33 #>      id title     year lda_medium_1 lda_medium_2 lda_medium_3 lda_medium_4 #>   <dbl> <fct>    <dbl>        <dbl>        <dbl>        <dbl>        <dbl> #> 1 21926 Proposa…  1990            0       0.0286            0        0.186 #> 2 20472 Michael   1990            0       0                 0        0     #> # ℹ 26 more variables: lda_medium_5 <dbl>, lda_medium_6 <dbl>, #> #   lda_medium_7 <dbl>, lda_medium_8 <dbl>, lda_medium_9 <dbl>, #> #   lda_medium_10 <dbl>, lda_medium_11 <dbl>, lda_medium_12 <dbl>, #> #   lda_medium_13 <dbl>, lda_medium_14 <dbl>, lda_medium_15 <dbl>, #> #   lda_artist_1 <dbl>, lda_artist_2 <dbl>, lda_artist_3 <dbl>, #> #   lda_artist_4 <dbl>, lda_artist_5 <dbl>, lda_artist_6 <dbl>, #> #   lda_artist_7 <dbl>, lda_artist_8 <dbl>, lda_artist_9 <dbl>, …"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":null,"dir":"Reference","previous_headings":"","what":"Lemmatization of Token Variables — step_lemma","title":"Lemmatization of Token Variables — step_lemma","text":"step_lemma creates specification recipe step extract lemmatization token variable.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lemmatization of Token Variables — step_lemma","text":"","code":"step_lemma(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   skip = FALSE,   id = rand_id(\"lemma\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lemmatization of Token Variables — step_lemma","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lemmatization of Token Variables — step_lemma","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Lemmatization of Token Variables — step_lemma","text":"stem perform lemmatization , rather lets extract lemma attribute token variable. able use step_lemma need use tokenization method includes lemmatization. Currently using \"spacyr\" engine step_tokenize() provides lemmatization works well step_lemma.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Lemmatization of Token Variables — step_lemma","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Lemmatization of Token Variables — step_lemma","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_lemma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lemmatization of Token Variables — step_lemma","text":"","code":"if (FALSE) { library(recipes)  short_data <- data.frame(text = c(   \"This is a short tale,\",   \"With many cats and ladies.\" ))  rec_spec <- recipe(~text, data = short_data) %>%   step_tokenize(text, engine = \"spacyr\") %>%   step_lemma(text) %>%   step_tf(text)  rec_prepped <- prep(rec_spec)  bake(rec_prepped, new_data = NULL) }"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate n-grams From Token Variables — step_ngram","title":"Generate n-grams From Token Variables — step_ngram","text":"step_ngram creates specification recipe step convert token variable token variable ngrams.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate n-grams From Token Variables — step_ngram","text":"","code":"step_ngram(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   num_tokens = 3L,   min_num_tokens = 3L,   delim = \"_\",   skip = FALSE,   id = rand_id(\"ngram\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate n-grams From Token Variables — step_ngram","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). num_tokens number tokens n-gram. must integer greater equal 1. Defaults 3. min_num_tokens minimum number tokens n-gram. must integer greater equal 1 smaller n. Defaults 3. delim separator words n-gram. Defaults \"_\". skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate n-grams From Token Variables — step_ngram","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate n-grams From Token Variables — step_ngram","text":"use step leave ordering tokens meaningless. min_num_tokens <  num_tokens tokens order increasing fashion respect number tokens n-gram. min_num_tokens = 1 num_tokens = 3 output contains 1-grams followed 2-grams followed 3-grams.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Generate n-grams From Token Variables — step_ngram","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Generate n-grams From Token Variables — step_ngram","text":"step 1 tuning parameters: num_tokens: Number tokens (type: integer, default: 3)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Generate n-grams From Token Variables — step_ngram","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_ngram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate n-grams From Token Variables — step_ngram","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_ngram(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [6 tokens] #> 2 [1 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [1 tokens] #> # Unique Tokens: 1  tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  value id          #>   <chr>  <chr> <chr>       #> 1 medium NA    ngram_EteDH tidy(tate_obj, number = 2) #> # A tibble: 1 × 2 #>   terms  id          #>   <chr>  <chr>       #> 1 medium ngram_EteDH"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":null,"dir":"Reference","previous_headings":"","what":"Part of Speech Filtering of Token Variables — step_pos_filter","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"step_pos_filter creates specification recipe step filter token variable based part speech tags.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"","code":"step_pos_filter(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   keep_tags = \"NOUN\",   skip = FALSE,   id = rand_id(\"pos_filter\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). keep_tags Character variable part speech tags keep. See details complete list tags. Defaults \"NOUN\". skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"Possible part speech tags spacyr engine : \"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\" \"SPACE\". information look https://github.com/explosion/spaCy/blob/master/spacy/glossary.py.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"tidy() step, tibble columns terms (selectors variables selected) num_topics (number topics).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_pos_filter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Part of Speech Filtering of Token Variables — step_pos_filter","text":"","code":"if (FALSE) { library(recipes)  short_data <- data.frame(text = c(   \"This is a short tale,\",   \"With many cats and ladies.\" ))  rec_spec <- recipe(~text, data = short_data) %>%   step_tokenize(text, engine = \"spacyr\") %>%   step_pos_filter(text, keep_tags = \"NOUN\") %>%   step_tf(text)  rec_prepped <- prep(rec_spec)  bake(rec_prepped, new_data = NULL) }"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":null,"dir":"Reference","previous_headings":"","what":"Positional One-Hot encoding of Tokens — step_sequence_onehot","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"step_sequence_onehot creates specification recipe step take string one hot encoding character position.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"","code":"step_sequence_onehot(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   sequence_length = 100,   padding = \"pre\",   truncating = \"pre\",   vocabulary = NULL,   prefix = \"seq1hot\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"sequence_onehot\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"https://papers.nips.cc/paper/5782-character-level-convolutional-networks--text-classification.pdf","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). sequence_length numeric, number characters keep discarding. Defaults 100. padding 'pre' 'post', pad either sequence. defaults 'pre'. truncating 'pre' 'post', remove values sequences larger sequence_length either beginning end sequence. Defaults 'pre'. vocabulary character vector, characters mapped integers. Characters vocabulary encoded 0. Defaults letters. prefix prefix generated column names, default \"seq1hot\". keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"string capped sequence_length argument, strings shorter sequence_length padded empty characters. encoding assign integer character vocabulary, encode accordingly. Characters vocabulary encoded 0.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"tidy() step, tibble columns terms (selectors variables selected), vocabulary (index) token (text correspoding index).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_sequence_onehot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Positional One-Hot encoding of Tokens — step_sequence_onehot","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~medium, data = tate_text) %>%   step_tokenize(medium) %>%   step_tokenfilter(medium) %>%   step_sequence_onehot(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL) #> # A tibble: 4,284 × 100 #>    seq1hot_medium_1 seq1hot_medium_2 seq1hot_medium_3 seq1hot_medium_4 #>               <int>            <int>            <int>            <int> #>  1                0                0                0                0 #>  2                0                0                0                0 #>  3                0                0                0                0 #>  4                0                0                0                0 #>  5                0                0                0                0 #>  6                0                0                0                0 #>  7                0                0                0                0 #>  8                0                0                0                0 #>  9                0                0                0                0 #> 10                0                0                0                0 #> # ℹ 4,274 more rows #> # ℹ 96 more variables: seq1hot_medium_5 <int>, seq1hot_medium_6 <int>, #> #   seq1hot_medium_7 <int>, seq1hot_medium_8 <int>, #> #   seq1hot_medium_9 <int>, seq1hot_medium_10 <int>, #> #   seq1hot_medium_11 <int>, seq1hot_medium_12 <int>, #> #   seq1hot_medium_13 <int>, seq1hot_medium_14 <int>, #> #   seq1hot_medium_15 <int>, seq1hot_medium_16 <int>, …  tidy(tate_rec, number = 3) #> # A tibble: 1 × 4 #>   terms  vocabulary token id                    #>   <chr>  <chr>      <int> <chr>                 #> 1 medium NA            NA sequence_onehot_1vTqG tidy(tate_obj, number = 3) #> # A tibble: 100 × 4 #>    terms  vocabulary token     id                    #>    <chr>       <int> <chr>     <chr>                 #>  1 medium          1 16        sequence_onehot_1vTqG #>  2 medium          2 2         sequence_onehot_1vTqG #>  3 medium          3 3         sequence_onehot_1vTqG #>  4 medium          4 35        sequence_onehot_1vTqG #>  5 medium          5 4         sequence_onehot_1vTqG #>  6 medium          6 5         sequence_onehot_1vTqG #>  7 medium          7 6         sequence_onehot_1vTqG #>  8 medium          8 8         sequence_onehot_1vTqG #>  9 medium          9 acrylic   sequence_onehot_1vTqG #> 10 medium         10 aluminium sequence_onehot_1vTqG #> # ℹ 90 more rows"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":null,"dir":"Reference","previous_headings":"","what":"Stemming of Token Variables — step_stem","title":"Stemming of Token Variables — step_stem","text":"step_stem creates specification recipe step convert token variable stemmed version.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stemming of Token Variables — step_stem","text":"","code":"step_stem(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   options = list(),   custom_stemmer = NULL,   skip = FALSE,   id = rand_id(\"stem\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stemming of Token Variables — step_stem","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). options list options passed stemmer function. custom_stemmer custom stemming function. none provided default \"SnowballC\". skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stemming of Token Variables — step_stem","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Stemming of Token Variables — step_stem","text":"Words tend different forms depending context, organize, organizes, organizing. many situations beneficial words condensed one allow smaller pool words. Stemming act chopping end words using set heuristics. Note stemming done end word therefore work reliably ngrams sentences.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Stemming of Token Variables — step_stem","text":"tidy() step, tibble columns terms (selectors variables selected) is_custom_stemmer (indicate custom stemmer used).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Stemming of Token Variables — step_stem","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stem.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Stemming of Token Variables — step_stem","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stem(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [8 tokens] #> 2 [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3  tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  is_custom_stemmer id         #>   <chr>  <lgl>             <chr>      #> 1 medium FALSE             stem_nC7F0 tidy(tate_obj, number = 2) #> # A tibble: 1 × 3 #>   terms  is_custom_stemmer id         #>   <chr>  <lgl>             <chr>      #> 1 medium FALSE             stem_nC7F0  # Using custom stemmer. Here a custom stemmer that removes the last letter # if it is a \"s\". remove_s <- function(x) gsub(\"s$\", \"\", x)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stem(medium, custom_stemmer = remove_s)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [8 tokens] #> 2 [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":null,"dir":"Reference","previous_headings":"","what":"Filtering of Stop Words for Tokens Variables — step_stopwords","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"step_stopwords creates specification recipe step filter token variable stop words.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"","code":"step_stopwords(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   language = \"en\",   keep = FALSE,   stopword_source = \"snowball\",   custom_stopword_source = NULL,   skip = FALSE,   id = rand_id(\"stopwords\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). language character indicate language stop words ISO 639-1 coding scheme. keep logical. Specifies whether keep stop words discard . stopword_source character indicate stop words source listed stopwords::stopwords_getsources. custom_stopword_source character vector indicate custom list words cater users specific problem. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"Stop words words sometimes remove natural language processing tasks. stop words usually refers common words language universal stop word list. argument custom_stopword_source allows pass character vector filter . keep argument one can specify keep words instead removing thus allowing select words combination two arguments.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"tidy() step, tibble columns terms (selectors variables selected), value (name stop word list), keep (whether stop words removed kept).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_stopwords.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filtering of Stop Words for Tokens Variables — step_stopwords","text":"","code":"library(recipes) library(modeldata) data(tate_text) tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stopwords(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [6 tokens] #> 2 [2 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [2 tokens] #> # Unique Tokens: 2  tidy(tate_rec, number = 2) #> # A tibble: 1 × 4 #>   terms  value keep  id              #>   <chr>  <chr> <lgl> <chr>           #> 1 medium NA    NA    stopwords_RhquB tidy(tate_obj, number = 2) #> # A tibble: 1 × 4 #>   terms  value    keep  id              #>   <chr>  <chr>    <lgl> <chr>           #> 1 medium snowball FALSE stopwords_RhquB  # With a custom stop words list  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_stopwords(medium, custom_stopword_source = c(\"twice\", \"upon\")) tate_obj <- tate_rec %>%   prep(traimomg = tate_text)  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Normalization of Character Variables — step_text_normalization","title":"Normalization of Character Variables — step_text_normalization","text":"step_text_normalization creates specification recipe step perform Unicode Normalization character variables.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normalization of Character Variables — step_text_normalization","text":"","code":"step_text_normalization(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   normalization_form = \"nfc\",   skip = FALSE,   id = rand_id(\"text_normalization\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normalization of Character Variables — step_text_normalization","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). normalization_form single character string determining Unicode Normalization. Must one \"nfc\", \"nfd\", \"nfkd\", \"nfkc\", \"nfkc_casefold\". Defaults \"nfc\". See stringi::stri_trans_nfc() details. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normalization of Character Variables — step_text_normalization","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Normalization of Character Variables — step_text_normalization","text":"tidy() step, tibble columns terms (selectors variables selected) normalization_form (type normalization).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Normalization of Character Variables — step_text_normalization","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_text_normalization.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Normalization of Character Variables — step_text_normalization","text":"","code":"library(recipes)  sample_data <- tibble(text = c(\"sch\\U00f6n\", \"scho\\U0308n\"))  rec <- recipe(~., data = sample_data) %>%   step_text_normalization(text)  prepped <- rec %>%   prep()  bake(prepped, new_data = NULL, text) %>%   slice(1:2) #> # A tibble: 2 × 1 #>   text  #>   <fct> #> 1 schön #> 2 schön  bake(prepped, new_data = NULL) %>%   slice(2) %>%   pull(text) #> [1] schön #> Levels: schön  tidy(rec, number = 1) #> # A tibble: 1 × 3 #>   terms normalization_form id                       #>   <chr> <chr>              <chr>                    #> 1 text  NA                 text_normalization_3C6KE tidy(prepped, number = 1) #> # A tibble: 1 × 3 #>   terms normalization_form id                       #>   <chr> <chr>              <chr>                    #> 1 text  nfc                text_normalization_3C6KE"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Set of Text Features — step_textfeature","title":"Calculate Set of Text Features — step_textfeature","text":"step_textfeature creates specification recipe step extract number numeric features text column.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Set of Text Features — step_textfeature","text":"","code":"step_textfeature(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   extract_functions = textfeatures::count_functions,   prefix = \"textfeature\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"textfeature\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Set of Text Features — step_textfeature","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). extract_functions named list feature extracting functions. default count_functions textfeatures package. See details information. prefix prefix generated column names, default \"textfeature\". keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Set of Text Features — step_textfeature","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Set of Text Features — step_textfeature","text":"step take character column returns number numeric columns equal number functions list passed extract_functions argument. default list functions textfeatures package. functions passed extract_functions must take character vector input return numeric vector length, otherwise error thrown.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Calculate Set of Text Features — step_textfeature","text":"tidy() step, tibble columns terms (selectors variables selected) functions (name feature functions).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Calculate Set of Text Features — step_textfeature","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_textfeature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Set of Text Features — step_textfeature","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_textfeature(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL) %>%   slice(1:2) #> # A tibble: 2 × 31 #>      id artist   title  year textfeature_medium_n…¹ textfeature_medium_n…² #>   <dbl> <fct>    <fct> <dbl>                  <int>                  <int> #> 1 21926 Absalon  Prop…  1990                      8                      8 #> 2 20472 Auerbac… Mich…  1990                      3                      3 #> # ℹ abbreviated names: ¹​textfeature_medium_n_words, #> #   ²​textfeature_medium_n_uq_words #> # ℹ 25 more variables: textfeature_medium_n_charS <int>, #> #   textfeature_medium_n_uq_charS <int>, #> #   textfeature_medium_n_digits <int>, #> #   textfeature_medium_n_hashtags <int>, #> #   textfeature_medium_n_uq_hashtags <int>, …  bake(tate_obj, new_data = NULL) %>%   pull(textfeature_medium_n_words) #>    [1]  8  3  3  3  4  4  4  3  6  3  3  3  3  3  3  3  6  7  9  4  4  3 #>   [23]  3  3  3  3  3  3  3  5  8  4  4  3  3  3  3  3  3  1  5  4  1 10 #>   [45]  3  3  3  3  3  3  3  3  3  5  9  5  6  6  4  4  4  4  6  3  3  3 #>   [67]  3  4  3  6  3  3  3  5 10  3  3  4 15  3  8  6 10 12  5  3  4  3 #>   [89]  3  3  3  3  3  3  3  4  3  3  3  3  5  3  5  3  3  3  4  5  6  3 #>  [111]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #>  [133]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #>  [155]  3  3  3  3  3  6  3  6  6  1  5  8  1  4  6  6  6  4  7  5  3  3 #>  [177]  3  3  7  3  3  3  3  6  3  3  6  4  8 11  4 11  3  3  4  3  6  6 #>  [199]  3  3  6  3  6  8  7  5  6  3  6  5  5  3  5  5  3  3  3  3  3  3 #>  [221]  7  7  7  7  7  7  7  7  5  5  3  3  3  3  3  3  3  3  3  3 19  6 #>  [243]  3  3  3  6  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  3 #>  [265]  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  5  5 #>  [287]  5  5  5  3  6  7  7  6  6  4  4  4  4  4  4  4  4  6  6 10  3  3 #>  [309]  4  4  7  3  3  6  1 14 14 10  9  6  9  4  9  4 13  3  6  6  4  4 #>  [331]  4  4  5 11  4  4  8  9 11  5  3  3  3  3  3  3  3  3  3  3  3  3 #>  [353]  4  5  3  4 12 10  3  3  3  3  3  3  3  3  1  1  3  3  3  8  6  5 #>  [375]  8  6  5  5  5  5  7  3  3  3  3  3  2  7  7  7  4  4  3  5  5  5 #>  [397]  5  5  5  6  6  6  6  6  5  6  7  9  6  2  8  3  5  4  2 17 18  4 #>  [419]  3  3  3 10  3  4  3  3 10  7  5  7  3  5  7  5  5  7  5  5  5  7 #>  [441]  3  5  5  7  5  8  5  5  5  5  6  8  8  9  6  6  6  6  4  3  6  6 #>  [463]  4  4  1  3  3  3  3  3  3  3  3  3  3  3  1  9  3  3  4  4  6  6 #>  [485]  8  9  3  6  3  3  3  3  3  3  3  3  3  3  3  3  4  3  8 15 13 18 #>  [507] 12 12  3  6  6  6 11 10  3 13  4  4  3  3 10  9 13  4  7  6  4  3 #>  [529]  4  4  4  3 14  8  3  3  3  3  3  8  3  3  3  3  3  3  3  3  3  3 #>  [551]  4  4  3  3  3  3  3  3  3  3  3  3 11  6  3  6  6  1  4  7  7  3 #>  [573]  5  3  7  4  9  3  3  3  3  3  3  3  3  4  4  3  3  3  3  3  3  3 #>  [595]  6  6  6  6  5  8  3  2 10  3  5  4  3  3  3  6 17  4  1  3  3  3 #>  [617]  3  3  3  3  3  3  4  6  8  9  6  6  6  5  3  4  3  5  6  6  6  6 #>  [639]  6  6  5  2  7  6  7  7  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #>  [661]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  7  8  7  6 #>  [683]  6  4  5  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #>  [705]  3  3  4  7  5  5  5  7 10 11  9  4  4  7  3  3  3  3  3  3  7  6 #>  [727] 16 10 15  3  3  3  3  6  3 11  2  3  6  8 10  4  7 11 12  4  4  4 #>  [749]  3  3  3  3  4  6  8  6  1  5  7  7  7  8 11 11 11  5  5  5  3  3 #>  [771]  3  3  3  4  3  3  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #>  [793]  3  4  9  8  8  8  8  8  8  8  8  8  8  8  8  8  6 13  5  4  3  3 #>  [815]  3  3  3  3  1  9  4  3  6 11 10 11  4  3  6  3  5  8  8  7  6  6 #>  [837]  6  6  6  6  6  7  7  7  9  6  9  8  3  3  3  6  3  3  3  2  8  4 #>  [859]  4  5  3  3  3  3  3  3  3  3  3  3  8  7  3  3  3  3  3  5  5  3 #>  [881]  3  3  5  5  5  5  3  3  3 11  3  3  3  3  3  4  6  3  7  1  1  7 #>  [903]  4  5  4  4  4  4  3  3 11  3  4 10  3  3  3  3  3  5  8  3  6 10 #>  [925]  7  1  4  3  3  5  4 10  4  3  8  3  7  7  4  4 12  3  7  5  6  3 #>  [947]  3  3  4  6  7  3  3  3  3  3  3  3  3  3  3  3  3  6  5  1  5  3 #>  [969]  3  3  4  6  4  6  5  5  7  5  3  3  3  3  3  3  3  3  3  3  3  3 #>  [991]  3  3  3  3  6  2  3  6  4  6  6  5  6  6  2  5  5  5  5  5  4  9 #> [1013]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1035]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  3  3  3  3  3  3  3 #> [1057]  3  8  5  5  9  9 11 11 11  5  5  5  5  5  5  5  5  5  5  5  5  5 #> [1079]  5  5  5  5  5  5  5  5  5  9  5  5  5 11  5  5  5  5 18  6  6  4 #> [1101]  4  5  5  1  4  3  3  3  3  3  3  3  3  3  3  3  5  3  3  5  3  3 #> [1123]  5  3  3  5  3  5  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1145]  3  3  3  3  3  3  3  3  3  3  3  3  5  3  3  3  3  3  3  3  3  3 #> [1167]  3  3  3  3  3  3  4  1 14  7  2  8  5 11  3  3  3 11  3  7  3  4 #> [1189]  3  6  3  5  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 #> [1211]  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 #> [1233]  4  4  4  4  4  4  3  3  3  3  3  3  3  3  3  3  3  4  6  6  4  7 #> [1255]  7  7  7  6  3  4  9  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1277]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1299]  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4 10  4  3 #> [1321]  3  3  5  5  7  4  4  3  3  3  3  3  3  3  5  5  9  6  5  5  5  5 #> [1343]  5  5  5  5  7  6  4  4  7  4  4  7  5  9  9  9  6  6  5  5  6  5 #> [1365]  6  6  6  5  6  4  6  6  6  6  4  5  5  8  7  4  5 15  4  3  3  3 #> [1387]  3  3  3  3  3  5  3  5  5  5  5  4  4  6 15  4  6  5  5  5  5  5 #> [1409]  5  5  5  5  5  3  3  3  3  3  3  8  4  5  5  3  3  3  5  4  5  4 #> [1431]  8  1  4  4  4 12  8  3  3  3  3  3  3  3  6  5  3  5  3  5  5  3 #> [1453]  5  5  5  5  5  5  5  5  5  5 10 10  4  4  4  4  3  3  3  3  3  3 #> [1475]  3  3  3  3  3  3  8  8  5  5  5  5  5  5 13  3  3  3  3  3  3  3 #> [1497]  3  3  3  2  2  3  2  2  2  2 10  5  3  3  3  3  3  3  3  3  3  3 #> [1519]  3  4  4  4  4  4  3  1  7  4  5  3  5  3  5  5  3  5  5  3  5  5 #> [1541]  7  7  4  8  5  5  6  6  6 13  3  3  3  3  3  3  3  3  6  5  4  8 #> [1563]  9  8  8  9  7  8 10  8  6  8  6  6  6  7  6  6  6  6  6  5  5  5 #> [1585]  5  5  5  5  5  7  7  7  7  7  3  3  3  3  3  3  3  3  3  3  3  3 #> [1607] 14 13 10  4  3  8  3  5  4  4  4 18  7  3  3  3  3  3  3  3  3  6 #> [1629]  3  3  3  4  7  3  3  3  6  3  3  7  3  3  3  3  3  3  3  3  3  3 #> [1651]  3  3  5  6  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1673]  3  7  4  3  3  3  6  1  3  3  3  2  2  3  3  3  3  5  3  7  5  7 #> [1695]  7  5  6  5  6  5  7  6  5  3  4  9  4  4  3  4  3  4  6  6  3  6 #> [1717]  6  6  3  3  3  3  3  3  4  4  6  3  6 12  3  3  3  3  3  3  6  4 #> [1739]  6  6  6  4  4 12  9  4  4  4  3  3  3  3  3  3  3  3  3  3  3  3 #> [1761]  3  5  4  3  3  3  3  3  4  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1783]  3  3  3  3  3  3  3  3  3  3  3  3  7  3  1  1  1  3  3  3  1  3 #> [1805]  3  3  3  3  5  3  3  4 16  6  6  6  6  6  5  6  6  6  6  6  6  6 #> [1827]  6  6  6  6  6 10  3  4  4  6  6  1  3  3  6  3  4  4  4  4  4  4 #> [1849]  4  4  6  3 10  8  6  3 13  7  6  6  6  6  6  6  6  6  6  6  6  6 #> [1871]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  4  6  6  4  4 #> [1893]  3  6  6  3  3  4  4  3  3  3  3  3  3  3  3  3  7  3  3  3  3  3 #> [1915]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  5  6  5  5  3  5  5  5 #> [1937]  5  5  3  5  5  6  6  6  6  6  6  6  6  6  9  3  3  3  3  3  6  3 #> [1959]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [1981]  3  3  3  4  4  4  4  1  4  6  6  3  9  9  9  3  3  4  5  6  6  6 #> [2003]  6  6  6  6  6  6  6  4  2  2  5  3  3  8  8  8  8  8  8  8  8  8 #> [2025]  8  8  8  3  3  3  3  3  3  3  3  3  3  3  3  8  9  9  9  3  7  2 #> [2047]  2  4  3  4  4  4  4  4  7  7  4  3  3  3  3  3  3  3  3  3  3  3 #> [2069]  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4 #> [2091]  4  4  2  5  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [2113]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [2135]  3  3  3  3  3  3  3  3  3  3  3  6  6  4  4  4  4  2  3  3  3  3 #> [2157]  3  3  3  3  4  4  7  3  3  3  4 12 10  3  4  7  6 17  7  6  6  6 #> [2179]  6  3  3  3  3  3  3  7  7  7  7  7  7  6  6  6  6  6  7 10 12  3 #> [2201]  5  6  6  4  6  5  4  4  3  3  8  5  2 11 10  9  9 11  8  7  7  7 #> [2223]  9  8  3  3 17  6  6  1  4  8  5  5  5  5  3  3  3  3  3  5  5  3 #> [2245]  5  5  3  3  3  3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5  5 #> [2267]  5  5  5  5  5  5  5  5  7  7  7  7  7  7  7 13  3  1 16 10  6  7 #> [2289]  3  5  6  4  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [2311]  3  3  7  3  4  8  3  3  6  3  4  7  8  2 10  6  6  1  1 10  9  9 #> [2333]  9  9  9  4  6  6  6  6  6  6  6  6  6  6  5  3 14  3  3  5  9  6 #> [2355]  6  6  4 12  7  7 11 11  8  6  3  3  3  3  3  3  3  3  3  3  3  3 #> [2377]  3  6  7  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  6  5  4  7 #> [2399]  3  4  4  6  6  6  7  6  7  7  7  7 12 19  3  5  5  5  5  5  5  5 #> [2421]  5  5  3  5  5  3  3  5  5  3  6  6  6  6  6  6  6  6  6  6  6  6 #> [2443]  6  6  6  6  6  6  6  6  2  7  4  5  3  3  3  3  3  3  3  3  3  3 #> [2465]  3  8  6  6  3  7  6  6 10  3  6  6 16  6  6  6  6  6  6 11  3  4 #> [2487]  4  3  3  3  3  3  3  3  8  6  3  3  3  3  6  4  6  6  6  6  6  6 #> [2509]  6  6  6  6  6  6  6  6  3  3  5  7  5  3  3  3  3  3  3  3  3  3 #> [2531]  3  3  3  3  3  3  3  3  3  3  3  9  9  3  9  8  6  3  3  3  3  1 #> [2553]  4  3  4  7  8  6  7  5  6  3  6  6  5  6  6  4 11  3  4  3  3  3 #> [2575]  6 12  4  4  3  3  3  3  3  3  3  6  4  3  3  3  3  3  3  3  3  3 #> [2597]  3  3  3  3  3  3  3  3  3  3  3 10  8  6  4  4  2  3  6  6 17  7 #> [2619]  5  5  3  3  3  3  3  3  3  3  3  3  3  3  3  5  4  6  6  5  6  6 #> [2641]  6  5  3  3  8 10  5  8  6  3  6  4  4  9  6 17  6  5  5  5  5  5 #> [2663]  5  5  5  5  5  5  5  5  4  3  3 11  3  3  3  3  3  3  3  3  3  3 #> [2685]  3  3  3  3  3  3  3  3  3  3  3  3  3  2  3  3  3  3  3  3  3  3 #> [2707]  3  3  5  4  4 16  6  4  6  4  9 11  6  3  3  3  8  8  3  8  8  8 #> [2729]  8  8  6  4  1  6  6  6  6  6  6  6  6  6  6  6  3  9  3  3  3  4 #> [2751]  3  6  6  6  6 10 10  9  3  6  6  4  3  7  4  7  5  5  5  7  5  9 #> [2773]  6 13  4  6  6  6  3  3  6  4  6  5  7 18  3  3  3  3  3  3  3  3 #> [2795]  3  3  4  4 17  5  3  3  3  3  3  3  5  3  3  7  7  7  7  7  6  6 #> [2817]  6  6  1  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [2839]  3  6 16  5 19  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 #> [2861]  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 #> [2883]  4  4  4  4 10  5 12 12 12  5 12  6  5  6  6  6  5  6  5  7  4 12 #> [2905]  8 10  3 11  2  6  6  6  6  4  4 11  5  6  1  5  6  6  2 10  4  4 #> [2927]  6  4  6  5  5  5  3  3  3  3  3  3  3  3  3  4  6  3  3  4  4  4 #> [2949]  4  1  5  3  3  6  9  6  6  6  3  3  3  3  3  3  3  3  3  3  3  3 #> [2971]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [2993]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [3015]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [3037]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [3059]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [3081]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3 #> [3103]  3  3  3  6 12  7  1 13  6  6  6  6  6  4  4  4  4  6  6  5 14  7 #> [3125]  5  4  3  3  3  3  3  4  4  4  4  4  4  1 16 17  6  3  3  5  4  4 #> [3147]  3  3  5  4  3  3 11  5  5  7  6  6  5  5  5  9  5 12  3  6  5  5 #> [3169]  5 11  1  1  5  6  6  3  3  9 10  3  4  3  3  3  3  3  3  3  3  3 #> [3191]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  7  9  3  3  3  3  3  3 #> [3213]  3  3  3  3  3  3  6  7  3  3  3  3  3  3  3  3  3  3  3  3  5  3 #> [3235]  3  3  3  6  5  3  3  3  3  3  3  3  3  3  3  3  3  3  3  5 10  7 #> [3257]  3  3  3  3  3  3  3  3  7  3  4  4  6  6  6  6  6  6  6  7  2  4 #> [3279]  8  3  3  3  3  3  6  1  5  6  6  6  6  6  6  3  7  7  3  3  3  3 #> [3301]  3  3  7  6  8  5  4  4  3  3  3  5  9  9  4  3  9  6  6  4  3  1 #> [3323]  5  5  5  5  5  5  7  1  3  9  3  9 15 15 12  8  5 19  4  4  7  7 #> [3345]  6 17  2  4  5  9  4  5  6  6  6  6  6  6  6  6  6  6  7  7  4  4 #> [3367]  4  6  6  6  6  6  6  6  6  6  3  6  3  5  3  3 16  5  3  8  3  3 #> [3389]  3  3  3  3  3  9  6  7 19  4  6  7  7 10  9  8  7  7  8  5  4 10 #> [3411]  4  3  3  5  4  5  5  6  6 14  7  8  8  8  8  8  8  8  8  8  4  4 #> [3433]  1  3 15  7  4  3  4  4  4  7  4  4 11  6  9  6  4  6  4  3  3  3 #> [3455]  3  3  4  3  4  6 15  7  3  3  3  9  3  3  3  3  3  4  5  5  5  1 #> [3477]  6  3  4  6  6  4  4  6  6  6  6  5  3 16 12  2 11  5  5  3 10 10 #> [3499]  2  3  3  3  5  5  5  5 17  5  8  9 15  3  3  3  6  3  3  3  3  3 #> [3521]  3  8  9  7 19 13  4  4  4 16  8  8 15  9  3  7  7  7  7  1 11 11 #> [3543]  7  3  3  3  4  4  4  4 15 15 15 15  8  8 13  1  4  8  8  7  7  6 #> [3565]  6  6  6  6  6  6  4  4  4  2  3  4  7  5  6  6  8  9  7  2  2  2 #> [3587]  4  3  4  6 10  6  6  6  6  5 10  7 13 11 11  3  5  8  4 15  8 19 #> [3609]  5  8  5  4  4  4  5  5  5  5  7  6  4  6  6  6  6  6  6  6 10  6 #> [3631]  5  5  6  3  3  6 11  4  5  4  4  4  4  5  5  7  4  4  4  4  4  4 #> [3653]  4  4  6  3  2  5 18 18 17  6  8  8  3  3  4  3  3  4  6  6 14  8 #> [3675]  5  7  6  7 12 10  4  6  6  6  8 10  7  4  4  4  4  4  4  8  8  5 #> [3697]  6  9  9  9  5  8  8  3  3  3  3  3  3  3  3  3  3  3  3  3  3  2 #> [3719] 18  5  8 11  4  3  3 11  4  9  9  4  6  9  4  7  4  8  5  6  6  6 #> [3741]  6  6  6  6  6  3  3  7  4  4  4  4  4  1  7  3  3  3  3  3  3  9 #> [3763]  5  4  4  4  4  4  4  4  3 11  8  8  6 10  4 13  3  3 12  4  5  3 #> [3785] 13  3  3  5  7 12  4  4 15  3  4  4  4  4  4  8  8  4  5  5  6  3 #> [3807]  6  6  6  6  5  5  4  6  3  4  6  6  7  4  8  7  7  7  7  7  7  7 #> [3829]  7  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  5  6  6  6  6  6 #> [3851]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 #> [3873]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 #> [3895]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 #> [3917]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 #> [3939]  6  6  6  6  6  6  6  6  6  4  4  4  3  3  3  4  1  3  3  3  6  6 #> [3961]  4 10  9  3  4  3  9  4  8  3  3  3  3  3  3  3  5  2  7  8  3  6 #> [3983]  6  3  3  8  6  7  4  7  7  7 15  7  5  7  3  5  3  6  7  4  4  4 #> [4005]  4  4  4  4  4  8  7  7  7  8  4  4  3  5  6  4  8  4  4  3  8  3 #> [4027]  4  7  3  3  3  5  4  5  3 10  5  7  6  3  3  4  4  4  4  4  5  4 #> [4049]  4  4  6 10  3  6 16  3 15  6  7  4  6 18  4 10  6  3  8 11  3  9 #> [4071] 11  8 16  4  5  6  6  6  5  4  4  6  5  1  6 10 12  9  6  6  3  7 #> [4093]  6  5  7  7  3  3  4  6  3  3  3  3  3  3  3  5  1  5 12 10  3  4 #> [4115]  8 11  8  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 #> [4137]  4  4  4  4  4  4  4  4  8  7  1  7  4  3  3  3  3  5  7  4  6  6 #> [4159]  6  6  6  6  6  6  6  6  4  3  3  5  3  3  3 16 10  3 17  5  5  5 #> [4181]  4  4  4  4  4 10  4  9  9  9  4  5  4  4  8  9  4  1  4  5  5  5 #> [4203]  6  5  5  4  8 10 12  4  9 15  3  4  4  4  4  3  1  3  7  7  7  7 #> [4225]  7  3  3  3  4  4  6  3  3  3  4  3  3  3  9 14  3  1  7  6  6  6 #> [4247]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 #> [4269]  6  6  6  6  7  6  6  6  6  6  6  6  6  6  6  4  tidy(tate_rec, number = 1) #> # A tibble: 1 × 3 #>   terms  functions id                #>   <chr>  <chr>     <chr>             #> 1 medium NA        textfeature_a9MZ0 tidy(tate_obj, number = 1) #> # A tibble: 27 × 3 #>    terms  functions     id                #>    <chr>  <chr>         <chr>             #>  1 medium n_words       textfeature_a9MZ0 #>  2 medium n_uq_words    textfeature_a9MZ0 #>  3 medium n_charS       textfeature_a9MZ0 #>  4 medium n_uq_charS    textfeature_a9MZ0 #>  5 medium n_digits      textfeature_a9MZ0 #>  6 medium n_hashtags    textfeature_a9MZ0 #>  7 medium n_uq_hashtags textfeature_a9MZ0 #>  8 medium n_mentions    textfeature_a9MZ0 #>  9 medium n_uq_mentions textfeature_a9MZ0 #> 10 medium n_commas      textfeature_a9MZ0 #> # ℹ 17 more rows  # Using custom extraction functions nchar_round_10 <- function(x) round(nchar(x) / 10) * 10  recipe(~., data = tate_text) %>%   step_textfeature(medium,     extract_functions = list(nchar10 = nchar_round_10)   ) %>%   prep() %>%   bake(new_data = NULL) #> # A tibble: 4,284 × 5 #>        id artist             title             year textfeature_medium_n…¹ #>     <dbl> <fct>              <fct>            <dbl>                  <dbl> #>  1  21926 Absalon            Proposals for a…  1990                     60 #>  2  20472 Auerbach, Frank    Michael           1990                     20 #>  3  20474 Auerbach, Frank    Geoffrey          1990                     20 #>  4  20473 Auerbach, Frank    Jake              1990                     20 #>  5  20513 Auerbach, Frank    To the Studios    1990                     20 #>  6  21389 Ayres, OBE Gillian Phaëthon          1990                     20 #>  7 121187 Barlow, Phyllida   Untitled          1990                     20 #>  8  19455 Baselitz, Georg    Green VIII        1990                     20 #>  9  20938 Beattie, Basil     Present Bound     1990                     30 #> 10 105941 Beuys, Joseph      Joseph Beuys: A…  1990                     10 #> # ℹ 4,274 more rows #> # ℹ abbreviated name: ¹​textfeature_medium_nchar10"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Hashing of Tokens — step_texthash","title":"Feature Hashing of Tokens — step_texthash","text":"step_texthash creates specification recipe step convert token variable multiple numeric variables using hashing trick.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Hashing of Tokens — step_texthash","text":"","code":"step_texthash(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   signed = TRUE,   num_terms = 1024L,   prefix = \"texthash\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"texthash\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Hashing of Tokens — step_texthash","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). signed logical, indicating whether use signed hash-function reduce collisions hashing. Defaults TRUE. num_terms integer, number variables output. Defaults 1024. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Feature Hashing of Tokens — step_texthash","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Feature Hashing of Tokens — step_texthash","text":"Feature hashing, hashing trick, transformation text variable new set numerical variables. done applying hashing function tokens using hash values feature indices. allows low memory representation text. implementation done using MurmurHash3 method. argument num_terms controls number indices hashing function map . tuning parameter transformation. Since hashing function can map two different tokens index, higher value num_terms result lower chance collision. new components names begin prefix, name variable, followed tokens separated -. variable names padded zeros. example prefix = \"hash\", num_terms < 10, names hash1 - hash9. num_terms = 101, names hash001 - hash101.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Feature Hashing of Tokens — step_texthash","text":"tidy() step, tibble columns terms (selectors variables selected) value (number terms).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Feature Hashing of Tokens — step_texthash","text":"step 2 tuning parameters: signed: Signed Hash Value (type: logical, default: TRUE) num_terms: # Hash Features (type: integer, default: 1024)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Feature Hashing of Tokens — step_texthash","text":"underlying operation allow case weights.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Feature Hashing of Tokens — step_texthash","text":"Kilian Weinberger; Anirban Dasgupta; John Langford; Alex Smola; Josh Attenberg (2009).","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_texthash.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Feature Hashing of Tokens — step_texthash","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_tokenfilter(medium, max_tokens = 10) %>%   step_texthash(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, tate_text) #> # A tibble: 4,284 × 1,028 #>    texthash_medium_0001 texthash_medium_0002 texthash_medium_0003 #>                   <int>                <int>                <int> #>  1                    0                    0                    0 #>  2                    0                    0                    0 #>  3                    0                    0                    0 #>  4                    0                    0                    0 #>  5                    0                    0                    0 #>  6                    0                    0                    0 #>  7                    0                    0                    0 #>  8                    0                    0                    0 #>  9                    0                    0                    0 #> 10                    0                    0                    0 #> # ℹ 4,274 more rows #> # ℹ 1,025 more variables: texthash_medium_0004 <int>, #> #   texthash_medium_0005 <int>, texthash_medium_0006 <int>, #> #   texthash_medium_0007 <int>, texthash_medium_0008 <int>, #> #   texthash_medium_0009 <int>, texthash_medium_0010 <int>, #> #   texthash_medium_0011 <int>, texthash_medium_0012 <int>, #> #   texthash_medium_0013 <int>, texthash_medium_0014 <int>, …  tidy(tate_rec, number = 3) #> # A tibble: 1 × 4 #>   terms  value length id             #>   <chr>  <lgl>  <int> <chr>          #> 1 medium NA        NA texthash_fdN6Z tidy(tate_obj, number = 3) #> # A tibble: 1 × 4 #>   terms  value length id             #>   <chr>  <lgl>  <int> <chr>          #> 1 medium TRUE    1024 texthash_fdN6Z"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":null,"dir":"Reference","previous_headings":"","what":"Term frequency of Tokens — step_tf","title":"Term frequency of Tokens — step_tf","text":"step_tf creates specification recipe step convert token variable multiple variables containing token counts.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Term frequency of Tokens — step_tf","text":"","code":"step_tf(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   weight_scheme = \"raw count\",   weight = 0.5,   vocabulary = NULL,   res = NULL,   prefix = \"tf\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"tf\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Term frequency of Tokens — step_tf","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). weight_scheme character determining weighting scheme term frequency calculations. Must one \"binary\", \"raw count\", \"term frequency\", \"log normalization\" \"double normalization\". Defaults \"raw count\". weight numeric weight used weight_scheme set \"double normalization\". Defaults 0.5. vocabulary character vector strings considered. res words used calculate term frequency stored preprocessing step trained prep.recipe(). prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Term frequency of Tokens — step_tf","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Term frequency of Tokens — step_tf","text":"strongly advised use step_tokenfilter using step_tf limit number variables created, otherwise might run memory issues. good strategy start low token count go according much RAM want use. Term frequency weight many times token appear observation. different ways calculate weight step can couple ways. Setting argument weight_scheme \"binary\" result set binary variables denoting token present observation. \"raw count\" count times token present observation. \"term frequency\" divide count total number words document limit effect document length longer documents tends word present times necessarily higher percentage. \"log normalization\" takes log 1 plus count, adding 1 done avoid taking log 0. Finally \"double normalization\" raw frequency divided raw frequency occurring term document. multiplied weight weight added result. done prevent bias towards longer documents. new components names begin prefix, name variable, followed tokens separated -. variable names padded zeros. example prefix = \"hash\", num_terms < 10, names hash1 - hash9. num_terms = 101, names hash001 - hash101.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Term frequency of Tokens — step_tf","text":"tidy() step, tibble columns terms (selectors variables selected) value (weighting scheme).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Term frequency of Tokens — step_tf","text":"step 2 tuning parameters: weight_scheme: Term Frequency Weight Method (type: character, default: raw count) weight: Weight (type: double, default: 0.5)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Term frequency of Tokens — step_tf","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Term frequency of Tokens — step_tf","text":"","code":"# \\donttest{ library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_tf(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, tate_text) #> # A tibble: 4,284 × 956 #>        id artist        title  year tf_medium_1 tf_medium_10 tf_medium_100 #>     <dbl> <fct>         <fct> <dbl>       <int>        <int>         <int> #>  1  21926 Absalon       Prop…  1990           0            0             0 #>  2  20472 Auerbach, Fr… Mich…  1990           0            0             0 #>  3  20474 Auerbach, Fr… Geof…  1990           0            0             0 #>  4  20473 Auerbach, Fr… Jake   1990           0            0             0 #>  5  20513 Auerbach, Fr… To t…  1990           0            0             0 #>  6  21389 Ayres, OBE G… Phaë…  1990           0            0             0 #>  7 121187 Barlow, Phyl… Unti…  1990           0            0             0 #>  8  19455 Baselitz, Ge… Gree…  1990           0            0             0 #>  9  20938 Beattie, Bas… Pres…  1990           0            0             0 #> 10 105941 Beuys, Joseph Jose…  1990           0            0             0 #> # ℹ 4,274 more rows #> # ℹ 949 more variables: tf_medium_11 <int>, tf_medium_12 <int>, #> #   tf_medium_13 <int>, tf_medium_133 <int>, tf_medium_14 <int>, #> #   tf_medium_15 <int>, tf_medium_151 <int>, tf_medium_16 <int>, #> #   tf_medium_160 <int>, tf_medium_16mm <int>, tf_medium_18 <int>, #> #   tf_medium_19 <int>, tf_medium_2 <int>, tf_medium_20 <int>, #> #   tf_medium_2000 <int>, tf_medium_201 <int>, tf_medium_21 <int>, …  tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  value id       #>   <chr>  <chr> <chr>    #> 1 medium NA    tf_1cXu4 tidy(tate_obj, number = 2) #> # A tibble: 1 × 3 #>   terms  value     id       #>   <chr>  <chr>     <chr>    #> 1 medium raw count tf_1cXu4 # }"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"step_tfidf creates specification recipe step convert token variable multiple variables containing term frequency-inverse document frequency tokens.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"","code":"step_tfidf(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   vocabulary = NULL,   res = NULL,   smooth_idf = TRUE,   norm = \"l1\",   sublinear_tf = FALSE,   prefix = \"tfidf\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"tfidf\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). vocabulary character vector strings considered. res words used calculate term frequency stored preprocessing step trained prep.recipe(). smooth_idf TRUE smooth IDF weights adding one document frequencies, extra document seen containing every term collection exactly . prevents division zero. norm character, defines type normalization apply term vectors. \"l1\" default, .e., scale number words document. Must one c(\"l1\", \"l2\", \"none\"). sublinear_tf logical, apply sublinear term-frequency scaling, .e., replace term frequency 1 + log(TF). Defaults FALSE. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"strongly advised use step_tokenfilter using step_tfidf limit number variables created; otherwise may run memory issues. good strategy start low token count increase depending much RAM want use. Term frequency-inverse document frequency product two statistics: term frequency (TF) inverse document frequency (IDF). Term frequency measures many times token appears observation. Inverse document frequency measure informative word , e.g., common rare word across observations. word appears observations might give much insight, appears might help differentiate observations. IDF defined follows: idf = log(1 + (# documents corpus) / (# documents term appears)) new components names begin prefix, name variable, followed tokens separated -. variable names padded zeros. example prefix = \"hash\", num_terms < 10, names hash1 - hash9. num_terms = 101, names hash001 - hash101.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"tidy() step, tibble columns terms (selectors variables selected), token (name tokens), weight (calculated IDF weight) returned.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tfidf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Term Frequency-Inverse Document Frequency of Tokens — step_tfidf","text":"","code":"# \\donttest{ library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_tfidf(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, tate_text) #> # A tibble: 4,284 × 956 #>        id artist             title     year tfidf_medium_1 tfidf_medium_10 #>     <dbl> <fct>              <fct>    <dbl>          <dbl>           <dbl> #>  1  21926 Absalon            Proposa…  1990              0               0 #>  2  20472 Auerbach, Frank    Michael   1990              0               0 #>  3  20474 Auerbach, Frank    Geoffrey  1990              0               0 #>  4  20473 Auerbach, Frank    Jake      1990              0               0 #>  5  20513 Auerbach, Frank    To the …  1990              0               0 #>  6  21389 Ayres, OBE Gillian Phaëthon  1990              0               0 #>  7 121187 Barlow, Phyllida   Untitled  1990              0               0 #>  8  19455 Baselitz, Georg    Green V…  1990              0               0 #>  9  20938 Beattie, Basil     Present…  1990              0               0 #> 10 105941 Beuys, Joseph      Joseph …  1990              0               0 #> # ℹ 4,274 more rows #> # ℹ 950 more variables: tfidf_medium_100 <dbl>, tfidf_medium_11 <dbl>, #> #   tfidf_medium_12 <dbl>, tfidf_medium_13 <dbl>, tfidf_medium_133 <dbl>, #> #   tfidf_medium_14 <dbl>, tfidf_medium_15 <dbl>, tfidf_medium_151 <dbl>, #> #   tfidf_medium_16 <dbl>, tfidf_medium_160 <dbl>, #> #   tfidf_medium_16mm <dbl>, tfidf_medium_18 <dbl>, #> #   tfidf_medium_19 <dbl>, tfidf_medium_2 <dbl>, tfidf_medium_20 <dbl>, …  tidy(tate_rec, number = 2) #> # A tibble: 1 × 4 #>   terms  token weight id          #>   <chr>  <chr>  <dbl> <chr>       #> 1 medium NA        NA tfidf_puv03 tidy(tate_obj, number = 2) #> # A tibble: 952 × 4 #>    terms  token weight id          #>    <chr>  <chr>  <dbl> <chr>       #>  1 medium 1       7.26 tfidf_puv03 #>  2 medium 10      7.26 tfidf_puv03 #>  3 medium 100     7.26 tfidf_puv03 #>  4 medium 11      7.67 tfidf_puv03 #>  5 medium 12      7.67 tfidf_puv03 #>  6 medium 13      8.36 tfidf_puv03 #>  7 medium 133     8.36 tfidf_puv03 #>  8 medium 14      6.75 tfidf_puv03 #>  9 medium 15      6.57 tfidf_puv03 #> 10 medium 151     8.36 tfidf_puv03 #> # ℹ 942 more rows # }"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter Tokens Based on Term Frequency — step_tokenfilter","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"step_tokenfilter creates specification recipe step convert token variable filtered based frequency.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"","code":"step_tokenfilter(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   max_times = Inf,   min_times = 0,   percentage = FALSE,   max_tokens = 100,   filter_fun = NULL,   res = NULL,   skip = FALSE,   id = rand_id(\"tokenfilter\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). max_times integer. Maximal number times word can appear getting removed. min_times integer. Minimum number times word can appear getting removed. percentage logical. max_times min_times interpreted percentage instead count. max_tokens integer. keep top max_tokens tokens filtering done max_times min_times. Defaults 100. filter_fun function. function take vector characters, return logical vector length. function applied observation data set. Defaults NULL. arguments ignored argument used. res words keep stored preprocessing step trained prep.recipe(). skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"step allow limit tokens looking filtering occurrence corpus. able exclude tokens appear many times times data. can specified counts using max_times min_times percentages setting percentage TRUE. addition one can filter use top max_tokens used tokens. max_tokens set Inf tokens used. generally lead large data sets tokens words trigrams. good strategy start low token count go according much RAM want use. strongly advised filter using step_tf step_tfidf limit number variables created.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"tidy() step, tibble columns terms (selectors variables selected) value (number unique tokens).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"step 3 tuning parameters: max_times: Maximum Token Frequency (type: integer, default: Inf) min_times: Minimum Token Frequency (type: integer, default: 0) max_tokens: # Retained Tokens (type: integer, default: 100)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenfilter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter Tokens Based on Term Frequency — step_tokenfilter","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_tokenfilter(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [8 tokens] #> 2 [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3  tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  value id                #>   <chr>  <int> <chr>             #> 1 medium    NA tokenfilter_KtTsR tidy(tate_obj, number = 2) #> # A tibble: 1 × 3 #>   terms  value id                #>   <chr>  <int> <chr>             #> 1 medium   952 tokenfilter_KtTsR"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Tokenization of Character Variables — step_tokenize","title":"Tokenization of Character Variables — step_tokenize","text":"step_tokenize() creates specification recipe step convert character predictor token variable.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tokenization of Character Variables — step_tokenize","text":"","code":"step_tokenize(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   training_options = list(),   options = list(),   token = \"words\",   engine = \"tokenizers\",   custom_token = NULL,   skip = FALSE,   id = rand_id(\"tokenize\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tokenization of Character Variables — step_tokenize","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). training_options list options passed tokenizer trained. applicable engine == \"tokenizers.bpe\". options list options passed tokenizer. token Unit tokenizing. See details options. Defaults \"words\". engine Package used tokenization. See details options. Defaults \"tokenizers\". custom_token User supplied tokenizer. Use argument overwrite token engine arguments. Must take character vector input output list character vectors. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tokenization of Character Variables — step_tokenize","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tokenization of Character Variables — step_tokenize","text":"Tokenization act splitting character string smaller parts analyzed. step uses tokenizers package includes heuristics split text paragraphs tokens, word tokens, among others. textrecipes keeps tokens token variable steps tasks token variable transforming back numeric variables. Working textrecipes almost always start calling step_tokenize followed modifying filtering steps. always case sometimes want apply pre-tokenization steps, can done recipes::step_mutate().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"engines","dir":"Reference","previous_headings":"","what":"Engines","title":"Tokenization of Character Variables — step_tokenize","text":"choice engine determines possible choices token. following small example data used following examples","code":"text_tibble <- tibble(   text = c(\"This is words\", \"They are nice!\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"tokenizers","dir":"Reference","previous_headings":"","what":"tokenizers","title":"Tokenization of Character Variables — step_tokenize","text":"tokenizers package default engine comes following unit token. options correspond function tokenizers package. \"words\" (default) \"characters\" \"character_shingles\" \"ngrams\" \"skip_ngrams\" \"sentences\" \"lines\" \"paragraphs\" \"regex\" \"ptb\" (Penn Treebank) \"skip_ngrams\" \"word_stems\" default tokenizer \"word\" splits text series words. using step_tokenize() without setting arguments get word tokens   tokenizer arguments change tokenization occurs can accessed using options argument passing named list. telling tokenizers::tokenize_words want turn words lowercase   can also stop removing punctuation.   tokenizer can changed setting different token. change return character tokens.   worth noting token methods appropriate included completeness.","code":"recipe(~ text, data = text_tibble) %>%   step_tokenize(text) %>%   show_tokens(text) #> [[1]] #> [1] \"this\"  \"is\"    \"words\" #>  #> [[2]] #> [1] \"they\" \"are\"  \"nice\" recipe(~ text, data = text_tibble) %>%   step_tokenize(text,                 options = list(lowercase = FALSE)) %>%   show_tokens(text) #> [[1]] #> [1] \"This\"  \"is\"    \"words\" #>  #> [[2]] #> [1] \"They\" \"are\"  \"nice\" recipe(~ text, data = text_tibble) %>%   step_tokenize(text,                 options = list(strip_punct = FALSE,                                lowercase = FALSE)) %>%   show_tokens(text) #> [[1]] #> [1] \"This\"  \"is\"    \"words\" #>  #> [[2]] #> [1] \"They\" \"are\"  \"nice\" \"!\" recipe(~ text, data = text_tibble) %>%   step_tokenize(text, token = \"characters\") %>%   show_tokens(text) #> [[1]] #>  [1] \"t\" \"h\" \"i\" \"s\" \"i\" \"s\" \"w\" \"o\" \"r\" \"d\" \"s\" #>  #> [[2]] #>  [1] \"t\" \"h\" \"e\" \"y\" \"a\" \"r\" \"e\" \"n\" \"i\" \"c\" \"e\""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"spacyr","dir":"Reference","previous_headings":"","what":"spacyr","title":"Tokenization of Character Variables — step_tokenize","text":"\"words\"","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"tokenizers-bpe","dir":"Reference","previous_headings":"","what":"tokenizers.bpe","title":"Tokenization of Character Variables — step_tokenize","text":"tokeenizers.bpe engine performs Byte Pair Encoding Text Tokenization. \"words\" tokenizer trained training set thus need passed training arguments. passed training_options argument important one vocab_size. determines number unique tokens tokenizer produce. generally set much higher value, typically thousands, set 22 demonstration purposes.","code":"recipe(~ text, data = text_tibble) %>%   step_tokenize(     text,     engine = \"tokenizers.bpe\",     training_options = list(vocab_size = 22)   ) %>%   show_tokens(text) #> [[1]] #>  [1] \"_Th\" \"is\"  \"_\"   \"is\"  \"_\"   \"w\"   \"o\"   \"r\"   \"d\"   \"s\"   #>  #> [[2]] #>  [1] \"_Th\" \"e\"   \"y\"   \"_\"   \"a\"   \"r\"   \"e\"   \"_\"   \"n\"   \"i\"   \"c\"   \"e\"   #> [13] \"!\""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"udpipe","dir":"Reference","previous_headings":"","what":"udpipe","title":"Tokenization of Character Variables — step_tokenize","text":"\"words\"","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"custom-token","dir":"Reference","previous_headings":"","what":"custom_token","title":"Tokenization of Character Variables — step_tokenize","text":"Sometimes need perform tokenization covered supported engines. case can use custom_token argument pass function performs tokenization want. example simple space tokenization. fast way tokenizing.","code":"space_tokenizer <- function(x) {   strsplit(x, \" +\") }  recipe(~ text, data = text_tibble) %>%   step_tokenize(     text,     custom_token = space_tokenizer   ) %>%   show_tokens(text) #> [[1]] #> [1] \"This\"  \"is\"    \"words\" #>  #> [[2]] #> [1] \"They\"  \"are\"   \"nice!\""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Tokenization of Character Variables — step_tokenize","text":"tidy() step, tibble columns terms (selectors variables selected) value (unit tokenization).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"tuning-parameters","dir":"Reference","previous_headings":"","what":"Tuning Parameters","title":"Tokenization of Character Variables — step_tokenize","text":"step 1 tuning parameters: token: Token Unit (type: character, default: words)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Tokenization of Character Variables — step_tokenize","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tokenization of Character Variables — step_tokenize","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [8 tokens] #> 2 [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3  tidy(tate_rec, number = 1) #> # A tibble: 1 × 3 #>   terms  value id             #>   <chr>  <chr> <chr>          #> 1 medium NA    tokenize_ahuiq tidy(tate_obj, number = 1) #> # A tibble: 1 × 3 #>   terms  value id             #>   <chr>  <chr> <chr>          #> 1 medium words tokenize_ahuiq  tate_obj_chars <- recipe(~., data = tate_text) %>%   step_tokenize(medium, token = \"characters\") %>%   prep()  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":null,"dir":"Reference","previous_headings":"","what":"BPE Tokenization of Character Variables — step_tokenize_bpe","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"step_tokenize_bpe() creates specification recipe step convert character predictor token variable using Byte Pair Encoding.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"","code":"step_tokenize_bpe(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   vocabulary_size = 1000,   options = list(),   res = NULL,   skip = FALSE,   id = rand_id(\"tokenize_bpe\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). vocabulary_size Integer, indicating number tokens final vocabulary. Defaults 1000. Highly encouraged tuned. options list options passed tokenizer. res fitted tokenizers.bpe::bpe() model tokenizer stored preprocessing step trained prep.recipe(). skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_bpe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"BPE Tokenization of Character Variables — step_tokenize_bpe","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize_bpe(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>       medium #>    <tknlist> #> 1 [8 tokens] #> 2 [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3  tidy(tate_rec, number = 1) #> # A tibble: 1 × 2 #>   terms  id                 #>   <chr>  <chr>              #> 1 medium tokenize_bpe_V6sSI tidy(tate_obj, number = 1) #> # A tibble: 1 × 2 #>   terms  id                 #>   <chr>  <chr>              #> 1 medium tokenize_bpe_V6sSI"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":null,"dir":"Reference","previous_headings":"","what":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"step_tokenize_sentencepiece() creates specification recipe step convert character predictor token variable using SentencePiece tokenization.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"","code":"step_tokenize_sentencepiece(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   vocabulary_size = 1000,   options = list(),   res = NULL,   skip = FALSE,   id = rand_id(\"tokenize_sentencepiece\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). vocabulary_size Integer, indicating number tokens final vocabulary. Defaults 1000. Highly encouraged tuned. options list options passed tokenizer. res fitted sentencepiece::sentencepiece() model tokenizer stored preprocessing step trained prep.recipe(). skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"running errors, can investigate progress compiled code setting options = list(verbose = TRUE). can reveal sentencepiece ran correctly .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_sentencepiece.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sentencepiece Tokenization of Character Variables — step_tokenize_sentencepiece","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize_sentencepiece(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>        medium #>     <tknlist> #> 1 [12 tokens] #> 2  [3 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [3 tokens] #> # Unique Tokens: 3  tidy(tate_rec, number = 1) #> # A tibble: 1 × 2 #>   terms  id                           #>   <chr>  <chr>                        #> 1 medium tokenize_sentencepiece_WATDS tidy(tate_obj, number = 1) #> # A tibble: 1 × 2 #>   terms  id                           #>   <chr>  <chr>                        #> 1 medium tokenize_sentencepiece_WATDS"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":null,"dir":"Reference","previous_headings":"","what":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"step_tokenize_wordpiece() creates specification recipe step convert character predictor token variable using WordPiece tokenization.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"","code":"step_tokenize_wordpiece(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   vocab = wordpiece::wordpiece_vocab(),   unk_token = \"[UNK]\",   max_chars = 100,   skip = FALSE,   id = rand_id(\"tokenize_wordpiece\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). vocab Character Character vector vocabulary tokens. Defaults wordpiece_vocab(). unk_token Token represent unknown words. Defaults \"[UNK]\". max_chars Integer, Maximum length word recognized. Defaults 100. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenize_wordpiece.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wordpiece Tokenization of Character Variables — step_tokenize_wordpiece","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize_wordpiece(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>        medium #>     <tknlist> #> 1 [12 tokens] #> 2  [4 tokens]  bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> <textrecipes_tokenlist[1]> #> [1] [4 tokens] #> # Unique Tokens: 4  tidy(tate_rec, number = 1) #> # A tibble: 1 × 2 #>   terms  id                       #>   <chr>  <chr>                    #> 1 medium tokenize_wordpiece_SyfJ9 tidy(tate_obj, number = 1) #> # A tibble: 1 × 2 #>   terms  id                       #>   <chr>  <chr>                    #> 1 medium tokenize_wordpiece_SyfJ9"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine Multiple Token Variables Into One — step_tokenmerge","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"step_tokenmerge creates specification recipe step take multiple token variables combine one token variable.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"","code":"step_tokenmerge(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   prefix = \"tokenmerge\",   skip = FALSE,   id = rand_id(\"tokenmerge\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). prefix prefix generated column names, default \"tokenmerge\". skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"tidy() step, tibble columns terms (selectors variables selected).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_tokenmerge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine Multiple Token Variables Into One — step_tokenmerge","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium, artist) %>%   step_tokenmerge(medium, artist)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL) #> # A tibble: 4,284 × 4 #>        id title                                            year tokenmerge #>     <dbl> <fct>                                           <dbl>  <tknlist> #>  1  21926 Proposals for a Habitat                          1990 [9 tokens] #>  2  20472 Michael                                          1990 [5 tokens] #>  3  20474 Geoffrey                                         1990 [5 tokens] #>  4  20473 Jake                                             1990 [5 tokens] #>  5  20513 To the Studios                                   1990 [6 tokens] #>  6  21389 Phaëthon                                         1990 [7 tokens] #>  7 121187 Untitled                                         1990 [6 tokens] #>  8  19455 Green VIII                                       1990 [5 tokens] #>  9  20938 Present Bound                                    1990 [8 tokens] #> 10 105941 Joseph Beuys: A Private Collection. A11 Artfor…  1990 [5 tokens] #> # ℹ 4,274 more rows  tidy(tate_rec, number = 2) #> # A tibble: 2 × 2 #>   terms  id               #>   <chr>  <chr>            #> 1 medium tokenmerge_bmWUo #> 2 artist tokenmerge_bmWUo tidy(tate_obj, number = 2) #> # A tibble: 2 × 2 #>   terms  id               #>   <chr>  <chr>            #> 1 medium tokenmerge_bmWUo #> 2 artist tokenmerge_bmWUo"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Untokenization of Token Variables — step_untokenize","title":"Untokenization of Token Variables — step_untokenize","text":"step_untokenize creates specification recipe step convert token variable character predictor.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Untokenization of Token Variables — step_untokenize","text":"","code":"step_untokenize(   recipe,   ...,   role = NA,   trained = FALSE,   columns = NULL,   sep = \" \",   skip = FALSE,   id = rand_id(\"untokenize\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Untokenization of Token Variables — step_untokenize","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role used step since new variables created. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). sep character determine tokens separated pasted together. Defaults \" \". skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Untokenization of Token Variables — step_untokenize","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Untokenization of Token Variables — step_untokenize","text":"steps turn token vector back character vector. step calling paste internally put tokens back together character.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Untokenization of Token Variables — step_untokenize","text":"tidy() step, tibble columns terms (selectors variables selected) value (seperator used collapsing).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Untokenization of Token Variables — step_untokenize","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_untokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Untokenization of Token Variables — step_untokenize","text":"","code":"library(recipes) library(modeldata) data(tate_text)  tate_rec <- recipe(~., data = tate_text) %>%   step_tokenize(medium) %>%   step_untokenize(medium)  tate_obj <- tate_rec %>%   prep()  bake(tate_obj, new_data = NULL, medium) %>%   slice(1:2) #> # A tibble: 2 × 1 #>   medium                                              #>   <fct>                                               #> 1 video monitor or projection colour and sound stereo #> 2 etching on paper                                     bake(tate_obj, new_data = NULL) %>%   slice(2) %>%   pull(medium) #> [1] etching on paper #> 1029 Levels: 100 digital prints on paper ink on paper and wall text ...  tidy(tate_rec, number = 2) #> # A tibble: 1 × 3 #>   terms  value id               #>   <chr>  <chr> <chr>            #> 1 medium NA    untokenize_9Smm1 tidy(tate_obj, number = 2) #> # A tibble: 1 × 3 #>   terms  value id               #>   <chr>  <chr> <chr>            #> 1 medium \" \"   untokenize_9Smm1"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Pretrained Word Embeddings of Tokens — step_word_embeddings","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"step_word_embeddings creates specification recipe step convert token variable word-embedding dimensions aggregating vectors token pre-trained embedding.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"","code":"step_word_embeddings(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   embeddings,   aggregation = c(\"sum\", \"mean\", \"min\", \"max\"),   aggregation_default = 0,   prefix = \"wordembed\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"word_embeddings\") )"},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See recipes::selections() details. role model terms created step, analysis role assigned?. default, function assumes new columns created original variables used predictors model. trained logical indicate quantities preprocessing estimated. columns character string variable names populated (eventually) terms argument. NULL step trained recipes::prep.recipe(). embeddings tibble pre-trained word embeddings, returned embedding_glove function textdata package. first column contain tokens, additional columns contain embeddings vectors. aggregation character giving name aggregation function use. Must one \"sum\", \"mean\", \"min\", \"max\". Defaults \"sum\". aggregation_default numeric denoting default value case words matched embedding. Defaults 0. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked recipes::bake.recipe()? operations baked recipes::prep.recipe() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = FALSE. id character string unique step identify .","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"updated version recipe new step added sequence existing steps ().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"Word embeddings map words (tokens) high-dimensional feature space. function maps pre-trained word embeddings onto tokens data. argument embeddings provides pre-trained vectors. dimension present tibble becomes new feature column, column aggregated across row text using function supplied aggregation argument. new components names begin prefix, name aggregation function, name variable embeddings tibble (usually something like \"d7\"). example, using default \"wordembedding\" prefix, GloVe embeddings textdata package (column names d1, d2, etc), new columns wordembedding_d1, wordembedding_d1, etc.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"tidying","dir":"Reference","previous_headings":"","what":"Tidying","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"tidy() step, tibble columns terms (selectors variables selected), embedding_rows (number rows embedding), aggregation (aggregation method).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"case-weights","dir":"Reference","previous_headings":"","what":"Case weights","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"underlying operation allow case weights.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/step_word_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pretrained Word Embeddings of Tokens — step_word_embeddings","text":"","code":"library(recipes)  embeddings <- tibble(   tokens = c(\"the\", \"cat\", \"ran\"),   d1 = c(1, 0, 0),   d2 = c(0, 1, 0),   d3 = c(0, 0, 1) )  sample_data <- tibble(   text = c(     \"The.\",     \"The cat.\",     \"The cat ran.\"   ),   text_label = c(\"fragment\", \"fragment\", \"sentence\") )  rec <- recipe(text_label ~ ., data = sample_data) %>%   step_tokenize(text) %>%   step_word_embeddings(text, embeddings = embeddings)  obj <- rec %>%   prep()  bake(obj, sample_data) #> # A tibble: 3 × 4 #>   text_label wordembed_text_d1 wordembed_text_d2 wordembed_text_d3 #>   <fct>                  <dbl>             <dbl>             <dbl> #> 1 fragment                   1                 0                 0 #> 2 fragment                   1                 1                 0 #> 3 sentence                   1                 1                 1  tidy(rec, number = 2) #> # A tibble: 1 × 4 #>   terms embeddings_rows aggregation id                    #>   <chr>           <int> <chr>       <chr>                 #> 1 text                3 sum         word_embeddings_F87sy tidy(obj, number = 2) #> # A tibble: 1 × 4 #>   terms embeddings_rows aggregation id                    #>   <chr>           <int> <chr>       <chr>                 #> 1 text                3 sum         word_embeddings_F87sy"},{"path":"https://textrecipes.tidymodels.org/dev/reference/textrecipes-package.html","id":null,"dir":"Reference","previous_headings":"","what":"textrecipes: Extra 'Recipes' for Text Processing — textrecipes-package","title":"textrecipes: Extra 'Recipes' for Text Processing — textrecipes-package","text":"Converting text numerical features requires specifically created procedures, implemented steps according 'recipes' package. steps allows tokenization, filtering, counting (tf tfidf) feature hashing.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/reference/textrecipes-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"textrecipes: Extra 'Recipes' for Text Processing — textrecipes-package","text":"Maintainer: Emil Hvitfeldt emil.hvitfeldt@posit.co (ORCID) contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tidy.recipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy the Result of a Recipe — tidy.step_clean_levels","title":"Tidy the Result of a Recipe — tidy.step_clean_levels","text":"tidy return data frame contains information regarding recipe operation within recipe (tidy method operation exists). See recipes::tidy.recipe information.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tidy.recipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidy the Result of a Recipe — tidy.step_clean_levels","text":"","code":"# S3 method for step_clean_levels tidy(x, ...)  # S3 method for step_clean_names tidy(x, ...)  # S3 method for step_dummy_hash tidy(x, ...)  # S3 method for step_lda tidy(x, ...)  # S3 method for step_lemma tidy(x, ...)  # S3 method for step_ngram tidy(x, ...)  # S3 method for step_pos_filter tidy(x, ...)  # S3 method for step_sequence_onehot tidy(x, ...)  # S3 method for step_stem tidy(x, ...)  # S3 method for step_stopwords tidy(x, ...)  # S3 method for step_text_normalization tidy(x, ...)  # S3 method for step_textfeature tidy(x, ...)  # S3 method for step_texthash tidy(x, ...)  # S3 method for step_tf tidy(x, ...)  # S3 method for step_tfidf tidy(x, ...)  # S3 method for step_tokenfilter tidy(x, ...)  # S3 method for step_tokenize tidy(x, ...)  # S3 method for step_tokenize_bpe tidy(x, ...)  # S3 method for step_tokenize_sentencepiece tidy(x, ...)  # S3 method for step_tokenize_wordpiece tidy(x, ...)  # S3 method for step_tokenmerge tidy(x, ...)  # S3 method for step_untokenize tidy(x, ...)  # S3 method for step_word_embeddings tidy(x, ...)"},{"path":"https://textrecipes.tidymodels.org/dev/reference/tidy.recipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidy the Result of a Recipe — tidy.step_clean_levels","text":"x step_word_embeddings object. ... currently used.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tokenlist.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Token Object — tokenlist","title":"Create Token Object — tokenlist","text":"tokenlist object thin wrapper around list character vectors, attributes.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tokenlist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Token Object — tokenlist","text":"","code":"tokenlist(tokens = list(), lemma = NULL, pos = NULL)"},{"path":"https://textrecipes.tidymodels.org/dev/reference/tokenlist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Token Object — tokenlist","text":"tokens List character vectors lemma List character vectors, must size shape x. pos List character vectors, must size shape x.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tokenlist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Token Object — tokenlist","text":"tokenlist object.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tokenlist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Token Object — tokenlist","text":"","code":"abc <- list(letters, LETTERS) tokenlist(abc) #> <textrecipes_tokenlist[2]> #> [1] [26 tokens] [26 tokens] #> # Unique Tokens: 52  unclass(tokenlist(abc)) #> $tokens #> $tokens[[1]] #>  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" #> [18] \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\" #>  #> $tokens[[2]] #>  [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" #> [18] \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\" #>  #>  #> attr(,\"unique_tokens\") #>  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" #> [18] \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\" \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" #> [35] \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" #> [52] \"Z\"  tibble(text = tokenlist(abc)) #> # A tibble: 2 × 1 #>          text #>     <tknlist> #> 1 [26 tokens] #> 2 [26 tokens]  library(tokenizers) library(modeldata) data(tate_text) tokens <- tokenize_words(as.character(tate_text$medium))  tokenlist(tokens) #> <textrecipes_tokenlist[4284]> #>    [1] [8 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #>    [6] [4 tokens]  [4 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  #>   [11] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>   [16] [3 tokens]  [6 tokens]  [7 tokens]  [10 tokens] [4 tokens]  #>   [21] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>   [26] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #>   [31] [8 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #>   [36] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [1 tokens]  #>   [41] [5 tokens]  [4 tokens]  [1 tokens]  [10 tokens] [3 tokens]  #>   [46] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>   [51] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [10 tokens] #>   [56] [5 tokens]  [6 tokens]  [7 tokens]  [4 tokens]  [4 tokens]  #>   [61] [4 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #>   [66] [3 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  [6 tokens]  #>   [71] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [11 tokens] #>   [76] [3 tokens]  [3 tokens]  [4 tokens]  [18 tokens] [3 tokens]  #>   [81] [8 tokens]  [6 tokens]  [10 tokens] [13 tokens] [5 tokens]  #>   [86] [3 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>   [91] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>   [96] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [101] [5 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #>  [106] [3 tokens]  [4 tokens]  [5 tokens]  [6 tokens]  [3 tokens]  #>  [111] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [116] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [121] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [126] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [131] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [136] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [141] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [146] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [151] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [156] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #>  [161] [3 tokens]  [6 tokens]  [6 tokens]  [1 tokens]  [5 tokens]  #>  [166] [8 tokens]  [1 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #>  [171] [6 tokens]  [4 tokens]  [8 tokens]  [5 tokens]  [3 tokens]  #>  [176] [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  [3 tokens]  #>  [181] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  #>  [186] [3 tokens]  [6 tokens]  [4 tokens]  [8 tokens]  [11 tokens] #>  [191] [4 tokens]  [11 tokens] [3 tokens]  [3 tokens]  [4 tokens]  #>  [196] [3 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #>  [201] [6 tokens]  [3 tokens]  [6 tokens]  [8 tokens]  [7 tokens]  #>  [206] [5 tokens]  [6 tokens]  [3 tokens]  [6 tokens]  [5 tokens]  #>  [211] [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #>  [216] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [221] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #>  [226] [7 tokens]  [7 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  #>  [231] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [236] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [241] [19 tokens] [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [246] [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [251] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [256] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [261] [3 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  #>  [266] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [271] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [276] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [281] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #>  [286] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #>  [291] [6 tokens]  [7 tokens]  [7 tokens]  [6 tokens]  [6 tokens]  #>  [296] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #>  [301] [4 tokens]  [4 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #>  [306] [10 tokens] [3 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  #>  [311] [7 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [1 tokens]  #>  [316] [14 tokens] [14 tokens] [11 tokens] [9 tokens]  [6 tokens]  #>  [321] [9 tokens]  [4 tokens]  [11 tokens] [4 tokens]  [14 tokens] #>  [326] [3 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  #>  [331] [4 tokens]  [4 tokens]  [6 tokens]  [12 tokens] [4 tokens]  #>  [336] [4 tokens]  [8 tokens]  [9 tokens]  [11 tokens] [5 tokens]  #>  [341] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [346] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [351] [3 tokens]  [3 tokens]  [4 tokens]  [5 tokens]  [3 tokens]  #>  [356] [4 tokens]  [13 tokens] [11 tokens] [3 tokens]  [3 tokens]  #>  [361] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [366] [3 tokens]  [1 tokens]  [1 tokens]  [3 tokens]  [3 tokens]  #>  [371] [3 tokens]  [8 tokens]  [6 tokens]  [5 tokens]  [9 tokens]  #>  [376] [6 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #>  [381] [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [386] [3 tokens]  [2 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #>  [391] [4 tokens]  [4 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  #>  [396] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [6 tokens]  #>  [401] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  #>  [406] [6 tokens]  [7 tokens]  [10 tokens] [6 tokens]  [2 tokens]  #>  [411] [8 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  [2 tokens]  #>  [416] [17 tokens] [18 tokens] [4 tokens]  [3 tokens]  [3 tokens]  #>  [421] [3 tokens]  [10 tokens] [3 tokens]  [4 tokens]  [3 tokens]  #>  [426] [3 tokens]  [11 tokens] [7 tokens]  [5 tokens]  [7 tokens]  #>  [431] [3 tokens]  [5 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  #>  [436] [7 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  #>  [441] [3 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  [5 tokens]  #>  [446] [8 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #>  [451] [6 tokens]  [8 tokens]  [8 tokens]  [9 tokens]  [6 tokens]  #>  [456] [6 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  [3 tokens]  #>  [461] [6 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  [1 tokens]  #>  [466] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [471] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [476] [3 tokens]  [1 tokens]  [9 tokens]  [3 tokens]  [3 tokens]  #>  [481] [4 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  [8 tokens]  #>  [486] [10 tokens] [3 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #>  [491] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [496] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [501] [4 tokens]  [3 tokens]  [8 tokens]  [15 tokens] [13 tokens] #>  [506] [18 tokens] [12 tokens] [12 tokens] [3 tokens]  [6 tokens]  #>  [511] [6 tokens]  [6 tokens]  [11 tokens] [10 tokens] [3 tokens]  #>  [516] [13 tokens] [5 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #>  [521] [10 tokens] [9 tokens]  [14 tokens] [4 tokens]  [8 tokens]  #>  [526] [6 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  #>  [531] [4 tokens]  [3 tokens]  [14 tokens] [8 tokens]  [3 tokens]  #>  [536] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #>  [541] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [546] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [551] [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [556] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [561] [3 tokens]  [3 tokens]  [12 tokens] [6 tokens]  [3 tokens]  #>  [566] [6 tokens]  [6 tokens]  [1 tokens]  [4 tokens]  [7 tokens]  #>  [571] [7 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [7 tokens]  #>  [576] [4 tokens]  [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [581] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [586] [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [591] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #>  [596] [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  [8 tokens]  #>  [601] [3 tokens]  [2 tokens]  [10 tokens] [3 tokens]  [5 tokens]  #>  [606] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #>  [611] [17 tokens] [4 tokens]  [1 tokens]  [3 tokens]  [3 tokens]  #>  [616] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [621] [3 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  [8 tokens]  #>  [626] [9 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  #>  [631] [3 tokens]  [4 tokens]  [3 tokens]  [5 tokens]  [7 tokens]  #>  [636] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #>  [641] [5 tokens]  [2 tokens]  [7 tokens]  [6 tokens]  [7 tokens]  #>  [646] [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [651] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [656] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [661] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [666] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [671] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [676] [3 tokens]  [3 tokens]  [4 tokens]  [7 tokens]  [8 tokens]  #>  [681] [7 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  [5 tokens]  #>  [686] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [691] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [696] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [701] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [706] [3 tokens]  [4 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  #>  [711] [5 tokens]  [7 tokens]  [10 tokens] [11 tokens] [9 tokens]  #>  [716] [4 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #>  [721] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #>  [726] [6 tokens]  [17 tokens] [10 tokens] [15 tokens] [3 tokens]  #>  [731] [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  [3 tokens]  #>  [736] [12 tokens] [2 tokens]  [4 tokens]  [6 tokens]  [8 tokens]  #>  [741] [11 tokens] [4 tokens]  [7 tokens]  [11 tokens] [13 tokens] #>  [746] [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #>  [751] [3 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  [8 tokens]  #>  [756] [6 tokens]  [1 tokens]  [5 tokens]  [7 tokens]  [7 tokens]  #>  [761] [7 tokens]  [8 tokens]  [11 tokens] [11 tokens] [11 tokens] #>  [766] [5 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #>  [771] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  #>  [776] [3 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [781] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [786] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [791] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [10 tokens] #>  [796] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #>  [801] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #>  [806] [8 tokens]  [8 tokens]  [8 tokens]  [6 tokens]  [14 tokens] #>  [811] [5 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [816] [3 tokens]  [3 tokens]  [3 tokens]  [1 tokens]  [9 tokens]  #>  [821] [4 tokens]  [3 tokens]  [6 tokens]  [11 tokens] [10 tokens] #>  [826] [11 tokens] [4 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  #>  [831] [5 tokens]  [8 tokens]  [8 tokens]  [7 tokens]  [6 tokens]  #>  [836] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #>  [841] [6 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [9 tokens]  #>  [846] [6 tokens]  [9 tokens]  [8 tokens]  [3 tokens]  [3 tokens]  #>  [851] [3 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [856] [2 tokens]  [9 tokens]  [4 tokens]  [4 tokens]  [5 tokens]  #>  [861] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [866] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [871] [8 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [876] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #>  [881] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #>  [886] [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [11 tokens] #>  [891] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [896] [4 tokens]  [6 tokens]  [3 tokens]  [7 tokens]  [1 tokens]  #>  [901] [1 tokens]  [7 tokens]  [4 tokens]  [5 tokens]  [4 tokens]  #>  [906] [4 tokens]  [4 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #>  [911] [13 tokens] [3 tokens]  [4 tokens]  [10 tokens] [3 tokens]  #>  [916] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #>  [921] [8 tokens]  [3 tokens]  [6 tokens]  [11 tokens] [7 tokens]  #>  [926] [1 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #>  [931] [4 tokens]  [10 tokens] [4 tokens]  [3 tokens]  [8 tokens]  #>  [936] [3 tokens]  [8 tokens]  [8 tokens]  [4 tokens]  [4 tokens]  #>  [941] [13 tokens] [3 tokens]  [7 tokens]  [5 tokens]  [6 tokens]  #>  [946] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  #>  [951] [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [956] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [961] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [5 tokens]  #>  [966] [1 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [971] [4 tokens]  [6 tokens]  [4 tokens]  [6 tokens]  [5 tokens]  #>  [976] [5 tokens]  [7 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #>  [981] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [986] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #>  [991] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  #>  [996] [2 tokens]  [3 tokens]  [6 tokens]  [4 tokens]  [7 tokens]  #> [1001] [6 tokens]  [5 tokens]  [7 tokens]  [6 tokens]  [2 tokens]  #> [1006] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1011] [4 tokens]  [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1016] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1021] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1026] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1031] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1036] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1041] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1046] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  #> [1051] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1056] [3 tokens]  [3 tokens]  [8 tokens]  [5 tokens]  [5 tokens]  #> [1061] [9 tokens]  [9 tokens]  [11 tokens] [11 tokens] [11 tokens] #> [1066] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1071] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1076] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1081] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1086] [5 tokens]  [5 tokens]  [9 tokens]  [5 tokens]  [5 tokens]  #> [1091] [5 tokens]  [11 tokens] [5 tokens]  [5 tokens]  [5 tokens]  #> [1096] [5 tokens]  [22 tokens] [6 tokens]  [6 tokens]  [4 tokens]  #> [1101] [4 tokens]  [5 tokens]  [5 tokens]  [1 tokens]  [4 tokens]  #> [1106] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1111] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1116] [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #> [1121] [3 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [1126] [5 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [1131] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1136] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1141] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1146] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1151] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1156] [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1161] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1166] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1171] [3 tokens]  [3 tokens]  [4 tokens]  [1 tokens]  [14 tokens] #> [1176] [7 tokens]  [2 tokens]  [9 tokens]  [5 tokens]  [11 tokens] #> [1181] [3 tokens]  [3 tokens]  [3 tokens]  [12 tokens] [3 tokens]  #> [1186] [7 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  [6 tokens]  #> [1191] [3 tokens]  [5 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1196] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1201] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1206] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1211] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1216] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1221] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1226] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1231] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1236] [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [1241] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1246] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #> [1251] [6 tokens]  [6 tokens]  [4 tokens]  [8 tokens]  [8 tokens]  #> [1256] [8 tokens]  [8 tokens]  [6 tokens]  [3 tokens]  [4 tokens]  #> [1261] [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1266] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1271] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1276] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1281] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1286] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1291] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1296] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1301] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1306] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [1311] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1316] [4 tokens]  [4 tokens]  [11 tokens] [4 tokens]  [3 tokens]  #> [1321] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  #> [1326] [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1331] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #> [1336] [5 tokens]  [9 tokens]  [6 tokens]  [5 tokens]  [5 tokens]  #> [1341] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1346] [5 tokens]  [7 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  #> [1351] [7 tokens]  [4 tokens]  [4 tokens]  [7 tokens]  [5 tokens]  #> [1356] [9 tokens]  [9 tokens]  [9 tokens]  [6 tokens]  [6 tokens]  #> [1361] [5 tokens]  [5 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  #> [1366] [6 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  [4 tokens]  #> [1371] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  #> [1376] [5 tokens]  [5 tokens]  [8 tokens]  [7 tokens]  [4 tokens]  #> [1381] [5 tokens]  [15 tokens] [4 tokens]  [3 tokens]  [3 tokens]  #> [1386] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1391] [3 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  #> [1396] [5 tokens]  [5 tokens]  [4 tokens]  [4 tokens]  [7 tokens]  #> [1401] [16 tokens] [4 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  #> [1406] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1411] [5 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [1416] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #> [1421] [4 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [1426] [3 tokens]  [5 tokens]  [4 tokens]  [5 tokens]  [4 tokens]  #> [1431] [8 tokens]  [1 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1436] [13 tokens] [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1441] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #> [1446] [5 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  #> [1451] [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1456] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1461] [5 tokens]  [5 tokens]  [10 tokens] [10 tokens] [4 tokens]  #> [1466] [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [1471] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1476] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1481] [8 tokens]  [8 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1486] [6 tokens]  [6 tokens]  [5 tokens]  [13 tokens] [3 tokens]  #> [1491] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1496] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [2 tokens]  #> [1501] [2 tokens]  [3 tokens]  [2 tokens]  [2 tokens]  [2 tokens]  #> [1506] [2 tokens]  [11 tokens] [5 tokens]  [3 tokens]  [3 tokens]  #> [1511] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1516] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [1521] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  #> [1526] [1 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #> [1531] [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #> [1536] [5 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  #> [1541] [7 tokens]  [7 tokens]  [4 tokens]  [8 tokens]  [5 tokens]  #> [1546] [5 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [13 tokens] #> [1551] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1556] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [5 tokens]  #> [1561] [5 tokens]  [8 tokens]  [9 tokens]  [8 tokens]  [8 tokens]  #> [1566] [9 tokens]  [7 tokens]  [8 tokens]  [10 tokens] [8 tokens]  #> [1571] [6 tokens]  [8 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1576] [7 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1581] [6 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [1586] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  #> [1591] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [3 tokens]  #> [1596] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1601] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1606] [3 tokens]  [14 tokens] [13 tokens] [10 tokens] [4 tokens]  #> [1611] [3 tokens]  [8 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [1616] [4 tokens]  [4 tokens]  [20 tokens] [7 tokens]  [3 tokens]  #> [1621] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1626] [3 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #> [1631] [3 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #> [1636] [3 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  #> [1641] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1646] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1651] [3 tokens]  [3 tokens]  [5 tokens]  [6 tokens]  [1 tokens]  #> [1656] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1661] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1666] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1671] [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  [4 tokens]  #> [1676] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [1 tokens]  #> [1681] [3 tokens]  [3 tokens]  [3 tokens]  [2 tokens]  [2 tokens]  #> [1686] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #> [1691] [4 tokens]  [7 tokens]  [5 tokens]  [7 tokens]  [7 tokens]  #> [1696] [5 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  [5 tokens]  #> [1701] [7 tokens]  [6 tokens]  [5 tokens]  [3 tokens]  [4 tokens]  #> [1706] [9 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  #> [1711] [3 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  #> [1716] [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #> [1721] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [1726] [4 tokens]  [6 tokens]  [3 tokens]  [6 tokens]  [13 tokens] #> [1731] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1736] [3 tokens]  [6 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #> [1741] [6 tokens]  [4 tokens]  [4 tokens]  [12 tokens] [10 tokens] #> [1746] [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [1751] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1756] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1761] [3 tokens]  [5 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [1766] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [3 tokens]  #> [1771] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1776] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1781] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1786] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1791] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [9 tokens]  #> [1796] [3 tokens]  [1 tokens]  [1 tokens]  [1 tokens]  [3 tokens]  #> [1801] [3 tokens]  [3 tokens]  [1 tokens]  [3 tokens]  [3 tokens]  #> [1806] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  #> [1811] [3 tokens]  [4 tokens]  [16 tokens] [6 tokens]  [6 tokens]  #> [1816] [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  #> [1821] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1826] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1831] [6 tokens]  [10 tokens] [3 tokens]  [4 tokens]  [4 tokens]  #> [1836] [6 tokens]  [6 tokens]  [1 tokens]  [3 tokens]  [3 tokens]  #> [1841] [6 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1846] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [1851] [6 tokens]  [3 tokens]  [10 tokens] [8 tokens]  [6 tokens]  #> [1856] [3 tokens]  [13 tokens] [7 tokens]  [6 tokens]  [6 tokens]  #> [1861] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1866] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1871] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1876] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1881] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1886] [6 tokens]  [6 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #> [1891] [4 tokens]  [4 tokens]  [3 tokens]  [6 tokens]  [6 tokens]  #> [1896] [3 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  #> [1901] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1906] [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  [3 tokens]  #> [1911] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1916] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1921] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1926] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [6 tokens]  #> [1931] [5 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  #> [1936] [5 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  #> [1941] [5 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1946] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [1951] [10 tokens] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1956] [3 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1961] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1966] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1971] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1976] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [1981] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  #> [1986] [4 tokens]  [4 tokens]  [1 tokens]  [4 tokens]  [6 tokens]  #> [1991] [6 tokens]  [3 tokens]  [9 tokens]  [9 tokens]  [9 tokens]  #> [1996] [3 tokens]  [3 tokens]  [4 tokens]  [5 tokens]  [6 tokens]  #> [2001] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2006] [6 tokens]  [6 tokens]  [7 tokens]  [6 tokens]  [4 tokens]  #> [2011] [2 tokens]  [2 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [2016] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [2021] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [2026] [8 tokens]  [8 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2031] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2036] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #> [2041] [9 tokens]  [9 tokens]  [9 tokens]  [3 tokens]  [7 tokens]  #> [2046] [2 tokens]  [2 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  #> [2051] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [7 tokens]  #> [2056] [7 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2061] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2066] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2071] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2076] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  #> [2081] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2086] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2091] [4 tokens]  [4 tokens]  [2 tokens]  [5 tokens]  [3 tokens]  #> [2096] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2101] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2106] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2111] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2116] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2121] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2126] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2131] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2136] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2141] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2146] [6 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2151] [4 tokens]  [2 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2156] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2161] [4 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #> [2166] [3 tokens]  [4 tokens]  [12 tokens] [10 tokens] [3 tokens]  #> [2171] [4 tokens]  [7 tokens]  [6 tokens]  [17 tokens] [7 tokens]  #> [2176] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  #> [2181] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2186] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [2191] [7 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2196] [6 tokens]  [7 tokens]  [10 tokens] [12 tokens] [3 tokens]  #> [2201] [5 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  [6 tokens]  #> [2206] [5 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [2211] [8 tokens]  [5 tokens]  [3 tokens]  [11 tokens] [10 tokens] #> [2216] [9 tokens]  [9 tokens]  [11 tokens] [8 tokens]  [7 tokens]  #> [2221] [7 tokens]  [7 tokens]  [9 tokens]  [8 tokens]  [3 tokens]  #> [2226] [3 tokens]  [18 tokens] [6 tokens]  [6 tokens]  [1 tokens]  #> [2231] [4 tokens]  [8 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2236] [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2241] [3 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  #> [2246] [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2251] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2256] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2261] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2266] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2271] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  #> [2276] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [2281] [7 tokens]  [15 tokens] [3 tokens]  [1 tokens]  [21 tokens] #> [2286] [10 tokens] [7 tokens]  [7 tokens]  [3 tokens]  [6 tokens]  #> [2291] [6 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [2296] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2301] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2306] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2311] [3 tokens]  [3 tokens]  [7 tokens]  [3 tokens]  [4 tokens]  #> [2316] [9 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  #> [2321] [4 tokens]  [7 tokens]  [8 tokens]  [2 tokens]  [10 tokens] #> [2326] [6 tokens]  [6 tokens]  [1 tokens]  [1 tokens]  [11 tokens] #> [2331] [9 tokens]  [9 tokens]  [9 tokens]  [9 tokens]  [9 tokens]  #> [2336] [4 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [2341] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [2346] [7 tokens]  [5 tokens]  [3 tokens]  [14 tokens] [3 tokens]  #> [2351] [3 tokens]  [5 tokens]  [10 tokens] [6 tokens]  [6 tokens]  #> [2356] [7 tokens]  [4 tokens]  [14 tokens] [7 tokens]  [7 tokens]  #> [2361] [11 tokens] [11 tokens] [8 tokens]  [7 tokens]  [3 tokens]  #> [2366] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2371] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2376] [3 tokens]  [3 tokens]  [6 tokens]  [7 tokens]  [3 tokens]  #> [2381] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2386] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2391] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #> [2396] [5 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [4 tokens]  #> [2401] [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [7 tokens]  #> [2406] [6 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [2411] [12 tokens] [20 tokens] [3 tokens]  [5 tokens]  [5 tokens]  #> [2416] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2421] [5 tokens]  [5 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  #> [2426] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [3 tokens]  #> [2431] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2436] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2441] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2446] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2451] [2 tokens]  [7 tokens]  [4 tokens]  [5 tokens]  [3 tokens]  #> [2456] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2461] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [2466] [8 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [7 tokens]  #> [2471] [6 tokens]  [6 tokens]  [12 tokens] [3 tokens]  [6 tokens]  #> [2476] [6 tokens]  [17 tokens] [6 tokens]  [6 tokens]  [6 tokens]  #> [2481] [6 tokens]  [6 tokens]  [6 tokens]  [11 tokens] [3 tokens]  #> [2486] [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2491] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #> [2496] [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2501] [6 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2506] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2511] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2516] [6 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [7 tokens]  #> [2521] [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2526] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2531] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2536] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2541] [3 tokens]  [9 tokens]  [9 tokens]  [3 tokens]  [9 tokens]  #> [2546] [8 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2551] [3 tokens]  [1 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  #> [2556] [8 tokens]  [8 tokens]  [6 tokens]  [7 tokens]  [5 tokens]  #> [2561] [6 tokens]  [3 tokens]  [6 tokens]  [7 tokens]  [5 tokens]  #> [2566] [6 tokens]  [6 tokens]  [4 tokens]  [11 tokens] [3 tokens]  #> [2571] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #> [2576] [12 tokens] [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [2581] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2586] [6 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2591] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2596] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2601] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2606] [3 tokens]  [3 tokens]  [10 tokens] [8 tokens]  [6 tokens]  #> [2611] [4 tokens]  [4 tokens]  [2 tokens]  [3 tokens]  [6 tokens]  #> [2616] [6 tokens]  [17 tokens] [7 tokens]  [5 tokens]  [5 tokens]  #> [2621] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2626] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2631] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [2636] [6 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  #> [2641] [6 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [9 tokens]  #> [2646] [10 tokens] [5 tokens]  [10 tokens] [6 tokens]  [3 tokens]  #> [2651] [6 tokens]  [4 tokens]  [4 tokens]  [9 tokens]  [6 tokens]  #> [2656] [17 tokens] [7 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2661] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2666] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [2671] [4 tokens]  [3 tokens]  [3 tokens]  [11 tokens] [3 tokens]  #> [2676] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2681] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2686] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2691] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2696] [3 tokens]  [3 tokens]  [2 tokens]  [3 tokens]  [3 tokens]  #> [2701] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2706] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [2711] [4 tokens]  [17 tokens] [7 tokens]  [4 tokens]  [6 tokens]  #> [2716] [4 tokens]  [9 tokens]  [11 tokens] [6 tokens]  [3 tokens]  #> [2721] [3 tokens]  [3 tokens]  [8 tokens]  [8 tokens]  [3 tokens]  #> [2726] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [2731] [6 tokens]  [4 tokens]  [1 tokens]  [6 tokens]  [6 tokens]  #> [2736] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2741] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  #> [2746] [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [2751] [3 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [2756] [10 tokens] [10 tokens] [9 tokens]  [3 tokens]  [6 tokens]  #> [2761] [6 tokens]  [4 tokens]  [3 tokens]  [7 tokens]  [4 tokens]  #> [2766] [7 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  #> [2771] [5 tokens]  [9 tokens]  [6 tokens]  [13 tokens] [4 tokens]  #> [2776] [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #> [2781] [6 tokens]  [4 tokens]  [6 tokens]  [5 tokens]  [7 tokens]  #> [2786] [18 tokens] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2791] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2796] [3 tokens]  [4 tokens]  [4 tokens]  [19 tokens] [5 tokens]  #> [2801] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2806] [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  #> [2811] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [6 tokens]  #> [2816] [6 tokens]  [6 tokens]  [6 tokens]  [1 tokens]  [4 tokens]  #> [2821] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2826] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2831] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2836] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [7 tokens]  #> [2841] [17 tokens] [5 tokens]  [19 tokens] [4 tokens]  [4 tokens]  #> [2846] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2851] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2856] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2861] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2866] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2871] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2876] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2881] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [2886] [4 tokens]  [10 tokens] [5 tokens]  [13 tokens] [13 tokens] #> [2891] [13 tokens] [5 tokens]  [13 tokens] [6 tokens]  [5 tokens]  #> [2896] [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  [6 tokens]  #> [2901] [5 tokens]  [7 tokens]  [4 tokens]  [12 tokens] [10 tokens] #> [2906] [10 tokens] [4 tokens]  [12 tokens] [2 tokens]  [6 tokens]  #> [2911] [6 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  #> [2916] [11 tokens] [5 tokens]  [6 tokens]  [1 tokens]  [5 tokens]  #> [2921] [6 tokens]  [6 tokens]  [2 tokens]  [10 tokens] [4 tokens]  #> [2926] [4 tokens]  [7 tokens]  [4 tokens]  [7 tokens]  [5 tokens]  #> [2931] [5 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2936] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2941] [3 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #> [2946] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [1 tokens]  #> [2951] [5 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [10 tokens] #> [2956] [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #> [2961] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2966] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2971] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2976] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2981] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2986] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2991] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [2996] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3001] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3006] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3011] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3016] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3021] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3026] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3031] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3036] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3041] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3046] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3051] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3056] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3061] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3066] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3071] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3076] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3081] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3086] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3091] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3096] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3101] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3106] [6 tokens]  [14 tokens] [8 tokens]  [1 tokens]  [13 tokens] #> [3111] [7 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3116] [4 tokens]  [4 tokens]  [4 tokens]  [5 tokens]  [7 tokens]  #> [3121] [7 tokens]  [5 tokens]  [15 tokens] [9 tokens]  [5 tokens]  #> [3126] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3131] [3 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3136] [4 tokens]  [4 tokens]  [1 tokens]  [16 tokens] [17 tokens] #> [3141] [6 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [3146] [4 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [3151] [3 tokens]  [3 tokens]  [12 tokens] [5 tokens]  [6 tokens]  #> [3156] [7 tokens]  [7 tokens]  [7 tokens]  [5 tokens]  [6 tokens]  #> [3161] [5 tokens]  [9 tokens]  [6 tokens]  [12 tokens] [4 tokens]  #> [3166] [6 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [11 tokens] #> [3171] [1 tokens]  [1 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  #> [3176] [3 tokens]  [3 tokens]  [9 tokens]  [10 tokens] [3 tokens]  #> [3181] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3186] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3191] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3196] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3201] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [7 tokens]  #> [3206] [9 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3211] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3216] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [8 tokens]  #> [3221] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3226] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3231] [3 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [3 tokens]  #> [3236] [3 tokens]  [3 tokens]  [7 tokens]  [6 tokens]  [3 tokens]  #> [3241] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3246] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3251] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [11 tokens] #> [3256] [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3261] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [8 tokens]  #> [3266] [3 tokens]  [4 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #> [3271] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3276] [8 tokens]  [3 tokens]  [4 tokens]  [9 tokens]  [4 tokens]  #> [3281] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [6 tokens]  #> [3286] [1 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3291] [6 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [7 tokens]  #> [3296] [7 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3301] [3 tokens]  [3 tokens]  [7 tokens]  [6 tokens]  [8 tokens]  #> [3306] [5 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  #> [3311] [3 tokens]  [5 tokens]  [9 tokens]  [9 tokens]  [4 tokens]  #> [3316] [3 tokens]  [9 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  #> [3321] [3 tokens]  [1 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [3326] [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  [1 tokens]  #> [3331] [4 tokens]  [10 tokens] [3 tokens]  [9 tokens]  [15 tokens] #> [3336] [15 tokens] [12 tokens] [8 tokens]  [5 tokens]  [19 tokens] #> [3341] [4 tokens]  [4 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [3346] [20 tokens] [2 tokens]  [4 tokens]  [5 tokens]  [9 tokens]  #> [3351] [4 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3356] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3361] [6 tokens]  [6 tokens]  [7 tokens]  [7 tokens]  [4 tokens]  #> [3366] [4 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3371] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3376] [6 tokens]  [3 tokens]  [6 tokens]  [3 tokens]  [5 tokens]  #> [3381] [3 tokens]  [3 tokens]  [16 tokens] [5 tokens]  [3 tokens]  #> [3386] [8 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3391] [3 tokens]  [3 tokens]  [3 tokens]  [10 tokens] [6 tokens]  #> [3396] [7 tokens]  [19 tokens] [4 tokens]  [6 tokens]  [7 tokens]  #> [3401] [7 tokens]  [11 tokens] [9 tokens]  [9 tokens]  [9 tokens]  #> [3406] [8 tokens]  [9 tokens]  [8 tokens]  [4 tokens]  [11 tokens] #> [3411] [4 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [3416] [6 tokens]  [5 tokens]  [6 tokens]  [7 tokens]  [15 tokens] #> [3421] [7 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [3426] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [3431] [4 tokens]  [4 tokens]  [1 tokens]  [3 tokens]  [15 tokens] #> [3436] [10 tokens] [4 tokens]  [3 tokens]  [5 tokens]  [4 tokens]  #> [3441] [4 tokens]  [7 tokens]  [4 tokens]  [4 tokens]  [11 tokens] #> [3446] [6 tokens]  [10 tokens] [7 tokens]  [4 tokens]  [6 tokens]  #> [3451] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3456] [4 tokens]  [5 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  #> [3461] [15 tokens] [7 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [3466] [10 tokens] [4 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3471] [3 tokens]  [4 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [3476] [1 tokens]  [6 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  #> [3481] [7 tokens]  [4 tokens]  [4 tokens]  [6 tokens]  [6 tokens]  #> [3486] [6 tokens]  [6 tokens]  [5 tokens]  [3 tokens]  [19 tokens] #> [3491] [13 tokens] [2 tokens]  [11 tokens] [6 tokens]  [6 tokens]  #> [3496] [3 tokens]  [10 tokens] [10 tokens] [2 tokens]  [3 tokens]  #> [3501] [3 tokens]  [3 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [3506] [5 tokens]  [20 tokens] [5 tokens]  [8 tokens]  [9 tokens]  #> [3511] [16 tokens] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  #> [3516] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3521] [3 tokens]  [8 tokens]  [10 tokens] [7 tokens]  [20 tokens] #> [3526] [14 tokens] [4 tokens]  [4 tokens]  [4 tokens]  [16 tokens] #> [3531] [8 tokens]  [10 tokens] [16 tokens] [9 tokens]  [4 tokens]  #> [3536] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [1 tokens]  #> [3541] [11 tokens] [11 tokens] [7 tokens]  [3 tokens]  [3 tokens]  #> [3546] [3 tokens]  [4 tokens]  [5 tokens]  [4 tokens]  [4 tokens]  #> [3551] [15 tokens] [15 tokens] [15 tokens] [15 tokens] [8 tokens]  #> [3556] [8 tokens]  [13 tokens] [1 tokens]  [5 tokens]  [8 tokens]  #> [3561] [8 tokens]  [8 tokens]  [8 tokens]  [6 tokens]  [6 tokens]  #> [3566] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3571] [4 tokens]  [4 tokens]  [4 tokens]  [2 tokens]  [3 tokens]  #> [3576] [6 tokens]  [8 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  #> [3581] [8 tokens]  [9 tokens]  [7 tokens]  [2 tokens]  [2 tokens]  #> [3586] [2 tokens]  [4 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  #> [3591] [10 tokens] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3596] [5 tokens]  [10 tokens] [7 tokens]  [14 tokens] [11 tokens] #> [3601] [11 tokens] [3 tokens]  [5 tokens]  [8 tokens]  [4 tokens]  #> [3606] [16 tokens] [8 tokens]  [19 tokens] [8 tokens]  [8 tokens]  #> [3611] [5 tokens]  [4 tokens]  [5 tokens]  [5 tokens]  [6 tokens]  #> [3616] [5 tokens]  [5 tokens]  [5 tokens]  [7 tokens]  [6 tokens]  #> [3621] [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3626] [6 tokens]  [6 tokens]  [6 tokens]  [11 tokens] [6 tokens]  #> [3631] [5 tokens]  [5 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  #> [3636] [6 tokens]  [11 tokens] [4 tokens]  [6 tokens]  [5 tokens]  #> [3641] [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  [5 tokens]  #> [3646] [8 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3651] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [6 tokens]  #> [3656] [3 tokens]  [2 tokens]  [5 tokens]  [18 tokens] [17 tokens] #> [3661] [18 tokens] [6 tokens]  [8 tokens]  [8 tokens]  [3 tokens]  #> [3666] [3 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [3671] [6 tokens]  [6 tokens]  [17 tokens] [8 tokens]  [5 tokens]  #> [3676] [7 tokens]  [6 tokens]  [8 tokens]  [13 tokens] [10 tokens] #> [3681] [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [8 tokens]  #> [3686] [10 tokens] [8 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3691] [4 tokens]  [4 tokens]  [4 tokens]  [8 tokens]  [8 tokens]  #> [3696] [5 tokens]  [6 tokens]  [9 tokens]  [9 tokens]  [9 tokens]  #> [3701] [5 tokens]  [8 tokens]  [8 tokens]  [3 tokens]  [3 tokens]  #> [3706] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3711] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3716] [3 tokens]  [3 tokens]  [3 tokens]  [19 tokens] [5 tokens]  #> [3721] [8 tokens]  [11 tokens] [4 tokens]  [3 tokens]  [3 tokens]  #> [3726] [11 tokens] [4 tokens]  [9 tokens]  [9 tokens]  [4 tokens]  #> [3731] [7 tokens]  [11 tokens] [4 tokens]  [8 tokens]  [4 tokens]  #> [3736] [8 tokens]  [5 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3741] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3746] [3 tokens]  [3 tokens]  [7 tokens]  [5 tokens]  [5 tokens]  #> [3751] [5 tokens]  [5 tokens]  [4 tokens]  [1 tokens]  [8 tokens]  #> [3756] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3761] [4 tokens]  [11 tokens] [5 tokens]  [4 tokens]  [4 tokens]  #> [3766] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3771] [3 tokens]  [12 tokens] [8 tokens]  [8 tokens]  [6 tokens]  #> [3776] [11 tokens] [4 tokens]  [14 tokens] [3 tokens]  [3 tokens]  #> [3781] [12 tokens] [4 tokens]  [6 tokens]  [3 tokens]  [13 tokens] #> [3786] [3 tokens]  [3 tokens]  [5 tokens]  [8 tokens]  [12 tokens] #> [3791] [4 tokens]  [4 tokens]  [19 tokens] [4 tokens]  [4 tokens]  #> [3796] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [8 tokens]  #> [3801] [8 tokens]  [4 tokens]  [5 tokens]  [5 tokens]  [6 tokens]  #> [3806] [3 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3811] [5 tokens]  [5 tokens]  [4 tokens]  [6 tokens]  [4 tokens]  #> [3816] [4 tokens]  [7 tokens]  [6 tokens]  [7 tokens]  [4 tokens]  #> [3821] [9 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  #> [3826] [7 tokens]  [7 tokens]  [7 tokens]  [7 tokens]  [6 tokens]  #> [3831] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3836] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3841] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  #> [3846] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3851] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3856] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3861] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3866] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3871] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3876] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3881] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3886] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3891] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3896] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3901] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3906] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3911] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3916] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3921] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3926] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3931] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3936] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3941] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [3946] [6 tokens]  [6 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [3951] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [1 tokens]  #> [3956] [3 tokens]  [3 tokens]  [3 tokens]  [6 tokens]  [6 tokens]  #> [3961] [6 tokens]  [10 tokens] [9 tokens]  [3 tokens]  [4 tokens]  #> [3966] [3 tokens]  [11 tokens] [4 tokens]  [8 tokens]  [3 tokens]  #> [3971] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [3976] [3 tokens]  [5 tokens]  [2 tokens]  [7 tokens]  [8 tokens]  #> [3981] [3 tokens]  [6 tokens]  [6 tokens]  [3 tokens]  [3 tokens]  #> [3986] [8 tokens]  [6 tokens]  [7 tokens]  [4 tokens]  [7 tokens]  #> [3991] [7 tokens]  [7 tokens]  [15 tokens] [7 tokens]  [5 tokens]  #> [3996] [7 tokens]  [3 tokens]  [5 tokens]  [3 tokens]  [6 tokens]  #> [4001] [7 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4006] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [8 tokens]  #> [4011] [7 tokens]  [7 tokens]  [9 tokens]  [8 tokens]  [5 tokens]  #> [4016] [4 tokens]  [3 tokens]  [6 tokens]  [7 tokens]  [4 tokens]  #> [4021] [9 tokens]  [4 tokens]  [4 tokens]  [3 tokens]  [10 tokens] #> [4026] [3 tokens]  [4 tokens]  [7 tokens]  [3 tokens]  [3 tokens]  #> [4031] [3 tokens]  [7 tokens]  [4 tokens]  [5 tokens]  [3 tokens]  #> [4036] [10 tokens] [5 tokens]  [7 tokens]  [6 tokens]  [3 tokens]  #> [4041] [3 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4046] [4 tokens]  [5 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4051] [6 tokens]  [10 tokens] [3 tokens]  [6 tokens]  [16 tokens] #> [4056] [3 tokens]  [15 tokens] [6 tokens]  [7 tokens]  [4 tokens]  #> [4061] [6 tokens]  [19 tokens] [4 tokens]  [10 tokens] [6 tokens]  #> [4066] [3 tokens]  [8 tokens]  [11 tokens] [3 tokens]  [9 tokens]  #> [4071] [12 tokens] [8 tokens]  [17 tokens] [4 tokens]  [5 tokens]  #> [4076] [6 tokens]  [6 tokens]  [6 tokens]  [5 tokens]  [5 tokens]  #> [4081] [5 tokens]  [7 tokens]  [5 tokens]  [1 tokens]  [6 tokens]  #> [4086] [11 tokens] [14 tokens] [9 tokens]  [6 tokens]  [6 tokens]  #> [4091] [3 tokens]  [7 tokens]  [6 tokens]  [5 tokens]  [7 tokens]  #> [4096] [7 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [6 tokens]  #> [4101] [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  #> [4106] [3 tokens]  [3 tokens]  [5 tokens]  [1 tokens]  [6 tokens]  #> [4111] [12 tokens] [11 tokens] [3 tokens]  [4 tokens]  [8 tokens]  #> [4116] [11 tokens] [9 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4121] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4126] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4131] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4136] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4141] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [10 tokens] #> [4146] [7 tokens]  [1 tokens]  [7 tokens]  [4 tokens]  [3 tokens]  #> [4151] [3 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  [7 tokens]  #> [4156] [4 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4161] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4166] [6 tokens]  [4 tokens]  [3 tokens]  [3 tokens]  [5 tokens]  #> [4171] [3 tokens]  [3 tokens]  [3 tokens]  [16 tokens] [11 tokens] #> [4176] [3 tokens]  [17 tokens] [5 tokens]  [5 tokens]  [5 tokens]  #> [4181] [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  [4 tokens]  #> [4186] [10 tokens] [4 tokens]  [9 tokens]  [9 tokens]  [9 tokens]  #> [4191] [4 tokens]  [5 tokens]  [4 tokens]  [4 tokens]  [8 tokens]  #> [4196] [9 tokens]  [4 tokens]  [1 tokens]  [4 tokens]  [5 tokens]  #> [4201] [5 tokens]  [5 tokens]  [6 tokens]  [5 tokens]  [5 tokens]  #> [4206] [4 tokens]  [8 tokens]  [10 tokens] [12 tokens] [4 tokens]  #> [4211] [9 tokens]  [15 tokens] [3 tokens]  [4 tokens]  [4 tokens]  #> [4216] [4 tokens]  [4 tokens]  [3 tokens]  [1 tokens]  [3 tokens]  #> [4221] [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  [8 tokens]  #> [4226] [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  [4 tokens]  #> [4231] [6 tokens]  [3 tokens]  [3 tokens]  [3 tokens]  [4 tokens]  #> [4236] [3 tokens]  [3 tokens]  [3 tokens]  [9 tokens]  [14 tokens] #> [4241] [3 tokens]  [1 tokens]  [7 tokens]  [6 tokens]  [6 tokens]  #> [4246] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4251] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4256] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4261] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4266] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4271] [6 tokens]  [6 tokens]  [7 tokens]  [6 tokens]  [6 tokens]  #> [4276] [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  [6 tokens]  #> [4281] [6 tokens]  [6 tokens]  [6 tokens]  [4 tokens]  #> # Unique Tokens: 952"},{"path":"https://textrecipes.tidymodels.org/dev/reference/tunable_textrecipes.html","id":null,"dir":"Reference","previous_headings":"","what":"tunable methods for textrecipes — tunable.step_dummy_hash","title":"tunable methods for textrecipes — tunable.step_dummy_hash","text":"functions define parameters can tuned specific steps. also define recommended objects dials package can used generate new parameter values characteristics.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tunable_textrecipes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tunable methods for textrecipes — tunable.step_dummy_hash","text":"","code":"# S3 method for step_dummy_hash tunable(x, ...)  # S3 method for step_ngram tunable(x, ...)  # S3 method for step_texthash tunable(x, ...)  # S3 method for step_tf tunable(x, ...)  # S3 method for step_tokenfilter tunable(x, ...)  # S3 method for step_tokenize tunable(x, ...)"},{"path":"https://textrecipes.tidymodels.org/dev/reference/tunable_textrecipes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tunable methods for textrecipes — tunable.step_dummy_hash","text":"x recipe step object ... used.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/reference/tunable_textrecipes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"tunable methods for textrecipes — tunable.step_dummy_hash","text":"tibble object.","code":""},{"path":[]},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-103","dir":"Changelog","previous_headings":"","what":"textrecipes 1.0.3","title":"textrecipes 1.0.3","text":"CRAN release: 2023-04-14","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"improvements-1-0-3","dir":"Changelog","previous_headings":"","what":"Improvements","title":"textrecipes 1.0.3","text":"Steps tunable arguments now arguments listed documentation. steps add new columns now informatively error name collision occurs.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"bug-fixes-1-0-3","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"textrecipes 1.0.3","text":"Fixed bug step_tf() wasn’t tunable weight argument.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-102","dir":"Changelog","previous_headings":"","what":"textrecipes 1.0.2","title":"textrecipes 1.0.2","text":"CRAN release: 2022-12-21 Setting token = \"tweets\" step_tokenize() deprecated due tokenizers::tokenize_tweets() deprecated. (#209) step_sequence_onehot(), step_dummy_hash(), step_dummy_texthash() now return integers. step_tf() returns integer weight_scheme \"binary\" \"raw count\". steps now required_pkgs() methods.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-101","dir":"Changelog","previous_headings":"","what":"textrecipes 1.0.1","title":"textrecipes 1.0.1","text":"CRAN release: 2022-10-06 Examples longer include (require(...)) code.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-100","dir":"Changelog","previous_headings":"","what":"textrecipes 1.0.0","title":"textrecipes 1.0.0","text":"CRAN release: 2022-07-02 Indicate steps support case weights (none), align documentation packages.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-052","dir":"Changelog","previous_headings":"","what":"textrecipes 0.5.2","title":"textrecipes 0.5.2","text":"CRAN release: 2022-05-04 Remove use okc_text vignette Fix bug printing tokenlists","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-051","dir":"Changelog","previous_headings":"","what":"textrecipes 0.5.1","title":"textrecipes 0.5.1","text":"CRAN release: 2022-03-29 step_tfidf() now correctly saves idf values applies testing data set. tidy.step_tfidf() now returns calculated IDF weights.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-050","dir":"Changelog","previous_headings":"","what":"textrecipes 0.5.0","title":"textrecipes 0.5.0","text":"CRAN release: 2022-03-20","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"new-steps-0-5-0","dir":"Changelog","previous_headings":"","what":"New steps","title":"textrecipes 0.5.0","text":"step_dummy_hash() generates binary indicators (possibly signed) simple factor character vectors. step_tokenize() gotten couple cousin functions step_tokenize_bpe(), step_tokenize_sentencepiece() step_tokenize_wordpiece() wraps {tokenizers.bpe}, {sentencepiece} {wordpiece} respectively (#147).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"improvements-and-other-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Improvements and Other Changes","title":"textrecipes 0.5.0","text":"Added all_tokenized() all_tokenized_predictors() easily select tokenized columns (#132). Use show_tokens() easily debug recipe involving tokenization. Reorganize documentation recipe step tidy methods (#126). Steps now dedicated subsection detailing happens tidy() applied. (#163) recipe steps now officially support empty selections aligned dplyr packages use tidyselect (#141). step_ngram() given speed increase put line packages performance. step_tokenize() now try error vocabulary size low using engine = \"tokenizers.bpe\" (#119). Warning given step_tokenfilter() filtering failed apply now correctly refers right argument name (#137). step_tf() now returns 0 instead NaN aren’t tokens present (#118). step_tokenfilter() now new argument filter_fun takes function can used filter tokens. (#164) tidy.step_stem() now correctly shows custom stemmer used. Added keep_original_cols argument step_lda, step_texthash(), step_tf(), step_tfidf(), step_word_embeddings(), step_dummy_hash(), step_sequence_onehot(), step_textfeatures() (#139).","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"breaking-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"textrecipes 0.5.0","text":"Steps prefix argument now creates names according pattern prefix_variablename_name/number. (#124)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-041","dir":"Changelog","previous_headings":"","what":"textrecipes 0.4.1","title":"textrecipes 0.4.1","text":"CRAN release: 2021-07-11","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"bug-fixes-0-4-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"textrecipes 0.4.1","text":"Fixed bug step_tokenfilter() step_sequence_onehot() sometimes caused crashes R 4.1.0.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-040","dir":"Changelog","previous_headings":"","what":"textrecipes 0.4.0","title":"textrecipes 0.4.0","text":"CRAN release: 2020-11-12","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"breaking-changes-0-4-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"textrecipes 0.4.0","text":"step_lda() now takes tokenlist instead character variable. See readme detail.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"new-features-0-4-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"textrecipes 0.4.0","text":"step_sequence_onehot() now takes tokenlists input. added {tokenizers.bpe} engine step_tokenize(). added {udpipe} engine step_tokenize(). added new steps cleaning variable names levels {janitor}, step_clean_names() step_clean_levels(). (#101)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-030","dir":"Changelog","previous_headings":"","what":"textrecipes 0.3.0","title":"textrecipes 0.3.0","text":"CRAN release: 2020-07-08 stopwords package moved Imports Suggests. step_ngram() gained argument min_num_tokens able return multiple n-grams together. (#90) Adds step_text_normalization() perform unicode normalization character vectors. (#86)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-023","dir":"Changelog","previous_headings":"","what":"textrecipes 0.2.3","title":"textrecipes 0.2.3","text":"CRAN release: 2020-05-22","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-022","dir":"Changelog","previous_headings":"","what":"textrecipes 0.2.2","title":"textrecipes 0.2.2","text":"CRAN release: 2020-05-10 step_word_embeddings() got argument aggregation_default specify value cases words matches embedding.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-021","dir":"Changelog","previous_headings":"","what":"textrecipes 0.2.1","title":"textrecipes 0.2.1","text":"CRAN release: 2020-05-04","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-020","dir":"Changelog","previous_headings":"","what":"textrecipes 0.2.0","title":"textrecipes 0.2.0","text":"CRAN release: 2020-04-14 step_tokenize() got engine argument specify packages tokenizers tokenize. spacyr added engine step_tokenize(). step_lemma() added extract lemma attribute tokenlists. step_pos_filter() added allow filtering tokens bases pat speech tags. step_ngram() added generate ngrams tokenlists. step_stem() correctly uses options argument. (Thanks @grayskripko finding bug, #64)","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-010","dir":"Changelog","previous_headings":"","what":"textrecipes 0.1.0","title":"textrecipes 0.1.0","text":"CRAN release: 2020-03-05 step_word2vec() changed step_lda() reflect actually happening. step_word_embeddings() added. Allows use pre-trained word embeddings convert token columns vectors high-dimensional “meaning” space. (@jonthegeek, #20) text2vec changed Imports Suggests. textfeatures changed Imports Suggests. step_tfidf() calculations slightly changed due flaw original implementation https://github.com/dselivanov/text2vec/issues/280.","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-002","dir":"Changelog","previous_headings":"","what":"textrecipes 0.0.2","title":"textrecipes 0.0.2","text":"CRAN release: 2019-09-07 Custom stemming function can now used step_stem using custom_stemmer argument. step_textfeatures() added, allows multiple numerical features pulled text. step_sequence_onehot() added, allows one hot encoding sequences fixed width. step_word2vec() added, calculates word2vec dimensions. step_tokenmerge() added, combines multiple list columns one list-columns. step_texthash() now correctly accepts signed argument. Documentation improved showcase importance filtering tokens applying step_tf() step_tfidf().","code":""},{"path":"https://textrecipes.tidymodels.org/dev/news/index.html","id":"textrecipes-001","dir":"Changelog","previous_headings":"","what":"textrecipes 0.0.1","title":"textrecipes 0.0.1","text":"CRAN release: 2018-12-17 First CRAN version","code":""}]
